{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Model Test\n",
    "Uses a pre-trained model to make predictions for dates that occur after the last date included in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enables project modules auto reloading when changed\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks if GPU is available\n",
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adds the project root folder to sys.path so project modules can be found from the Jupyter notebook\n",
    "# Assumes this notebook is located 2 levels up from the project root folder\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "current_path = Path.cwd()\n",
    "project_root = str(current_path.parent.parent)\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "print(\"Updated PATH: \", sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import settings\n",
    "import apis.tiingo_api as tiingo\n",
    "\n",
    "secret_key= settings.get_secret(\"tiingo-key\")\n",
    "client = tiingo.TiingoAPI(secret_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### SET PARAMETERS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import utils.text_utils as tu\n",
    "import utils.list_utils as lu\n",
    "\n",
    "MODEL_NAME= f\"2024-10-24-1549-TSLA-predictUP-dates20190102-20231229-days3-down300-up300-in16-hid12-pos_weight9186-prec9580pct-fp25tp570-high_prec.pth\"\n",
    "\n",
    "START_DATE = datetime(2023, 8, 15)   # Train END_DATE = (2024, 1, 1) - PRICE_AVGS = 2584 \n",
    "START_DATE = datetime(2023, 10, 14)   # Train END_DATE = (2024, 1, 1) - PRICE_AVGS = 2584 \n",
    "# START_DATE = datetime(2023, 4, 21)   # PRICE_AVGS = 4181\n",
    "# START_DATE = datetime(2023, 1, 2)   # PRICE_AVGS = 6765\n",
    "END_DATE = datetime(2024, 7, 5)\n",
    "\n",
    "TICKERS= [\"NTLA\", \"BEAM\", \"CRSP\", \"PACB\", \"EDIT\", \"VERV\", \"PRME\"]\n",
    "TICKERS= [tu.extract_company(MODEL_NAME)]\n",
    "DATA_INTERVAL_MINUTES = 15   # (Set to 5 or 15)\n",
    "DATA_AFTER_HOURS = False\n",
    "\n",
    "DAYS_PREDICT = tu.extract_days(MODEL_NAME)\n",
    "DOWN_PCTS_PREDICT= tu.extract_pcts(MODEL_NAME, \"down\")\n",
    "UP_PCTS_PREDICT= tu.extract_pcts(MODEL_NAME, \"up\")\n",
    "\n",
    "PRICE_AVGS= [\n",
    "    2, \n",
    "    3, \n",
    "    5, \n",
    "    8, \n",
    "    13, \n",
    "    21, \n",
    "    34, \n",
    "    55, \n",
    "    89, \n",
    "    144, \n",
    "    233, \n",
    "    377, \n",
    "    610, \n",
    "    987, \n",
    "    1597, \n",
    "    2584,\n",
    "    # 4181, # 255 working days\n",
    "    # 6765 # 364 working days\n",
    "]\n",
    "\n",
    "\n",
    "VOLUME_AVGS= [\n",
    "    2, \n",
    "    3, \n",
    "    5, \n",
    "    8, \n",
    "    13, \n",
    "    21, \n",
    "    34\n",
    "]\n",
    "\n",
    "PREDICT_UP = tu.is_predict_up(MODEL_NAME)\n",
    "if PREDICT_UP:\n",
    "    INDEX_KEEP= 2\n",
    "    INDEX_REMOVE_A= 0\n",
    "    INDEX_REMOVE_B= 1\n",
    "else:\n",
    "    INDEX_KEEP= 0\n",
    "    INDEX_REMOVE_A= 1\n",
    "    INDEX_REMOVE_B= 2\n",
    "\n",
    "# If quotes every 15min there 26 per day if quotes every 5min there are 78 per day\n",
    "TICKS_IN_DAY = 26 if DATA_INTERVAL_MINUTES == 15 else 78\n",
    "# How many data ticks are inspecting to determine the if up or down by percentage \n",
    "TICKS_PREDICT= TICKS_IN_DAY * DAYS_PREDICT\n",
    "REACH_PCT= 0.95\n",
    "\n",
    "HIDDEN_UNITS=12\n",
    "\n",
    "TEST_THRESHOLD = 0.99\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "TICKERS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "# Load training data (!!!ONLY TO VERIFY):\n",
    "Use to verify results: compare with results obtain when training\n",
    "If don't need to verify training data Skip to step that load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### TRAINING DATA\n",
    "csv_data2019= client.download_ticker(TICKERS[0], datetime(2019, 1, 1), datetime(2020,1,1), DATA_INTERVAL_MINUTES, DATA_AFTER_HOURS)\n",
    "csv_data2020= client.download_ticker(TICKERS[0], datetime(2020, 1, 1), datetime(2021,1,1), DATA_INTERVAL_MINUTES, DATA_AFTER_HOURS)\n",
    "csv_data2021= client.download_ticker(TICKERS[0], datetime(2021, 1, 1), datetime(2022,1,1), DATA_INTERVAL_MINUTES, DATA_AFTER_HOURS)\n",
    "csv_data2022= client.download_ticker(TICKERS[0], datetime(2022, 1, 1), datetime(2023,1,1), DATA_INTERVAL_MINUTES, DATA_AFTER_HOURS)\n",
    "csv_data2023= client.download_ticker(TICKERS[0], datetime(2023, 1, 1), datetime(2024,1,1), DATA_INTERVAL_MINUTES, DATA_AFTER_HOURS)\n",
    "# csv_data2024= client.download_ticker(TICKERS[0], datetime(2024, 1, 1), datetime(2024,2,1), DATA_INTERVAL_MINUTES, DATA_AFTER_HOURS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import pandas as pd\n",
    "\n",
    "# ### TRAINING DATA\n",
    "df2019 = pd.read_csv(io.StringIO(csv_data2019))\n",
    "df2020 = pd.read_csv(io.StringIO(csv_data2020))\n",
    "df2021 = pd.read_csv(io.StringIO(csv_data2021))\n",
    "df2022 = pd.read_csv(io.StringIO(csv_data2022))\n",
    "df2023 = pd.read_csv(io.StringIO(csv_data2023))\n",
    "# df2024 = pd.read_csv(io.StringIO(csv_data2024))\n",
    "\n",
    "if not df2019.empty:\n",
    "    print(\"Concatenating from 2019\")\n",
    "    df = pd.concat([df2019, df2020, df2021, df2022, df2023], axis=0, ignore_index=True)\n",
    "elif not df2020.empty:\n",
    "    print(\"Concatenating from 2020\")\n",
    "    df = pd.concat([df2020, df2021, df2022, df2023], axis=0, ignore_index=True)\n",
    "else:\n",
    "    print(\"Concatenating from 2021\")\n",
    "    df = pd.concat([df2021, df2022, df2023], axis=0, ignore_index=True)\n",
    "\n",
    "# if not df2024.empty:\n",
    "#     print(\"Concatenating from 2024\")\n",
    "#     df = pd.concat([df, df2024], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## LOADING TEST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # For first prediction need signal_avg[-1]=2584 ticks -> 2584 / 26 ~ 100 days -> 100 * 7 / 5 / 30 = 4.6 months\n",
    "csv_data2023= client.download_ticker(TICKERS[0], datetime(2023, 8, 1), datetime(2023,12,31), DATA_INTERVAL_MINUTES, DATA_AFTER_HOURS)\n",
    "csv_data2024= client.download_ticker(TICKERS[0], datetime(2024, 1, 1), datetime(2024,5,25), DATA_INTERVAL_MINUTES, DATA_AFTER_HOURS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import pandas as pd\n",
    "\n",
    "### TEST PREDICTIONS ON RECENT DATA\n",
    "df2023 = pd.read_csv(io.StringIO(csv_data2023))\n",
    "df2024 = pd.read_csv(io.StringIO(csv_data2024))\n",
    "df = pd.concat([df2023, df2024], axis=0, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validates that data has been concatenated correctly = ordered ascending\n",
    "if df[\"date\"].is_monotonic_increasing and df[\"date\"].is_unique:\n",
    "    print(\"Correct: DataFrame is in ascending order.\")\n",
    "else:\n",
    "    print(\"Error: DataFrame is not in ascending order.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify expected dates correspond with the data you intend to use\n",
    "print(f\"Test data first:\\n{df[['date', 'close']][:5]}\")\n",
    "print(f\"Test data last:\\n{df[['date', 'close']][-5:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import classifiers.up_down_classifier as udc\n",
    "import classifiers.ewa_classifier as ec\n",
    "\n",
    "alpha= ec.calculate_ewa_alpha(TICKS_PREDICT, REACH_PCT)\n",
    "print(f\"alpha: {alpha:.4f} for window: {TICKS_PREDICT} and reach: {REACH_PCT}\")\n",
    "\n",
    "classes_calc = udc.UpsDownsClassifier(TICKS_PREDICT, DOWN_PCTS_PREDICT, UP_PCTS_PREDICT)\n",
    "\n",
    "close_prices = df['close'].astype(float).tolist()\n",
    "input_data= ec.calculate_ewas(close_prices, alpha)\n",
    "\n",
    "classes= classes_calc.classify(input_data)\n",
    "print(f\"Check correct '-1' point (window={TICKS_PREDICT}): {classes[-TICKS_PREDICT-1:-TICKS_PREDICT+1]}\")\n",
    "print(f\"prices vs input_data: {[(p, c) for p, c in zip(close_prices[2650:3000], input_data[2650:3000])]}\")\n",
    "print(f\"input_data vs classes: {[(p, c) for p, c in zip(input_data[2650:3000], classes[2650:3000])]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display histogram for the classes\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a histogram\n",
    "hist_values, bin_edges, _ = plt.hist(classes, bins=4, edgecolor='black')\n",
    "\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Data')\n",
    "\n",
    "# Display frequency on top of each bar\n",
    "for value, edge in zip(hist_values, bin_edges[:-1]):\n",
    "    plt.text(float(edge), float(value), str(int(value)), color='black')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show percentages of each class value\n",
    "lu.display_frequency_classes(classes, DOWN_PCTS_PREDICT, UP_PCTS_PREDICT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Display classes value changes over time (last 500 ticks)\n",
    "graph_ticks = 500\n",
    "x = range(len(classes[-graph_ticks:]))\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(x, classes[-graph_ticks:], linestyle='-')\n",
    "\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Class')\n",
    "plt.title('Plot of Classes')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate the signals as input for the neural network as proportions\n",
    "import preprocessing.proportions_calc as proportions\n",
    "\n",
    "signals_calculator = proportions.ProportionsCalc(PRICE_AVGS)\n",
    "\n",
    "proportions_avg = signals_calculator.calculate(close_prices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Prices length: {len(close_prices)}\")\n",
    "print(f\"Proportions length: {len(proportions_avg[-1])}\")\n",
    "\n",
    "print(f\"Last 10 close: {close_prices[-10:]}\")\n",
    "print(f\"Last 10 proportions(avg={PRICE_AVGS[0]}): {proportions_avg[0][-10:]}\")\n",
    "\n",
    "print(f\"Proportions avgs: Length: {len(PRICE_AVGS)} Last: {PRICE_AVGS[-1]}\")\n",
    "# At the end of the data, when less ticks than necessary no possible to predict so \"-1\" \n",
    "print(f\"Classes last non-negative-1: {classes[-TICKS_PREDICT-1:-TICKS_PREDICT+1]} len: {len(classes)}\")\n",
    "print(f\"Proportions first non-negative-1(avg={PRICE_AVGS[-1]}): {proportions_avg[-1][PRICE_AVGS[-1]-2:PRICE_AVGS[-1]]} len: {len(proportions_avg[-1])}\")\n",
    "print(f\"Proportions (avg={PRICE_AVGS[0]}) Min: {min(proportions_avg[0][PRICE_AVGS[0]-1:-TICKS_PREDICT-1])} Max: {max(proportions_avg[0][PRICE_AVGS[0]-1:-TICKS_PREDICT])}\")\n",
    "print(f\"Proportions (avg={PRICE_AVGS[-1]}) Min: {min(proportions_avg[-1][PRICE_AVGS[-1]-1:-TICKS_PREDICT-1])} Max: {max(proportions_avg[-1][PRICE_AVGS[-1]-1:-TICKS_PREDICT])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the \"-1\" from the proportions\n",
    "#   At the beging first signal_avg[-1] are \"-1\" (need previous values for first avg.)\n",
    "#   At the end decided not predict if period to predict is shorter\n",
    "targets = classes[PRICE_AVGS[-1]-1:-TICKS_PREDICT]\n",
    "inputs = []\n",
    "for proportion in proportions_avg:\n",
    "    proportion_cut= proportion[PRICE_AVGS[-1]-1:-TICKS_PREDICT]\n",
    "    print(proportion_cut[:2])\n",
    "    inputs.append(proportion_cut)\n",
    "\n",
    "print(f\"First target: {targets[0]} and last target: {targets[-1]}\")\n",
    "print(f\"Classes: {len(classes)} after cut to targets: {len(targets)}\")\n",
    "print(f\"Inputs len: {len(inputs[len(PRICE_AVGS)-1])}\")\n",
    "print(f\"Distinct targets: {list(set(targets))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print(\"Test data: '-1' removed from begining and end\")\n",
    "lu.display_frequency_classes(targets, DOWN_PCTS_PREDICT, UP_PCTS_PREDICT)\n",
    "\n",
    "targets_frequency = Counter(targets)\n",
    "print(\"VALIDATE removing should be POSITIVE?\")\n",
    "count_remove_a= targets_frequency[INDEX_REMOVE_A] - targets_frequency[INDEX_KEEP] + targets_frequency[INDEX_KEEP] //2\n",
    "count_remove_b= targets_frequency[INDEX_REMOVE_B] - targets_frequency[INDEX_KEEP] + targets_frequency[INDEX_KEEP] //2\n",
    "print(f\"Removing {INDEX_REMOVE_A}: {count_remove_a}\")\n",
    "print(f\"Removing {INDEX_REMOVE_B}: {count_remove_b}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "##### SET PARAMETERS\n",
    "###################\n",
    "# DECISION SET: REMOVING?\n",
    "indexes_remove_a= []\n",
    "# 2024-03-01 Do not remove anything\n",
    "# if count_remove_a > 0:\n",
    "#     indexes_remove_a = get_indexes_value(targets, index_remove_a, count_remove_a)\n",
    "\n",
    "# DECISION SET: REMOVING?\n",
    "indexes_remove_b= []\n",
    "# 2024-03-01 Do not remove anything\n",
    "# if count_remove_b > 0:\n",
    "#     indexes_remove_b = get_indexes_value(targets, index_remove_b, count_remove_b)\n",
    "\n",
    "indexes_remove= indexes_remove_a + indexes_remove_b\n",
    "targets_clean= lu.remove_indexes(targets, indexes_remove)\n",
    "\n",
    "lu.display_frequency_classes(targets_clean, DOWN_PCTS_PREDICT, UP_PCTS_PREDICT)\n",
    "print(f\"Targets len: {len(targets)} Targets clean: {len(targets_clean)} Difference: {len(targets)-len(targets_clean)}\")\n",
    "\n",
    "inputs_clean = [lu.remove_indexes(input, indexes_remove) for input in inputs]    \n",
    "print(f\"targets_clean positions(Keep={INDEX_KEEP})(First:{targets_clean.index(INDEX_KEEP)},Last:-{targets_clean[::-1].index(INDEX_KEEP)})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "# HERE NEW LOAD\n",
    "## Method that allows to obtain multiple stock data for multiple years with a single call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from apis.tiingo_api import TiingoAPI\n",
    "from data_sources.data_loader import DataLoader\n",
    "from data_sources.tiingo_repo import TiingoRepo\n",
    "\n",
    "tiingo_client = TiingoAPI(secret_key)\n",
    "tiingo_repo= TiingoRepo(\n",
    "    tiingo_client, \n",
    "    START_DATE, \n",
    "    END_DATE, \n",
    "    DATA_INTERVAL_MINUTES, \n",
    "    DATA_AFTER_HOURS,\n",
    "    wait_time=5)\n",
    "data_loader= DataLoader(tiingo_repo)\n",
    "new_data= data_loader.load_data(TICKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validates that data has been concatenated correctly = ordered ascending\n",
    "if new_data[0][\"date\"].is_monotonic_increasing and new_data[0][\"date\"].is_unique:\n",
    "    print(\"Correct: DataFrame is in ascending order.\")\n",
    "else:\n",
    "    print(\"Error: DataFrame is not in ascending order.\")\n",
    "\n",
    "print(f\"Data first:\\n{new_data[0][['date', 'close']][:5]}\")\n",
    "print(f\"Data last:\\n{new_data[0][['date', 'close']][-5:]}\")\n",
    "new_data[0].to_csv('output.csv', index=False)\n",
    "print(f\"Len struct_data[0]: {new_data[0].shape[0]}\")\n",
    "\n",
    "duplicate_count = new_data[0]['date'].duplicated().sum()\n",
    "print(f\"Number of duplicate values: {duplicate_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing.points_features_extractor_price_time import PointsFeaturesExtractorPriceTime\n",
    "from preprocessing.points_target_extractor import PointsTargetExtractor\n",
    "from preprocessing.features_targets_pair import FeaturesTargetsPair\n",
    "\n",
    "points_features_extractor = PointsFeaturesExtractorPriceTime(PRICE_AVGS)\n",
    "points_target_extractor = PointsTargetExtractor(TICKS_PREDICT, REACH_PCT, DOWN_PCTS_PREDICT, UP_PCTS_PREDICT)\n",
    "features_targets_pair = FeaturesTargetsPair(points_features_extractor, points_target_extractor)\n",
    "features, targets = features_targets_pair.align(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets 'index_keep' as target = 1 and rest of indexes to target=0\n",
    "targets_binary= lu.convert_binary(targets, INDEX_KEEP)\n",
    "print(f\"targets_binary First {targets_binary.index(True)} and Last(counting from end) {targets_binary[::-1].index(True)} position with True\")\n",
    "print(f\"targets_binary len: {len(targets_binary)} Features first: {len(features[0])} Features last: {len(features[-1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "features_tensor = torch.Tensor(features)\n",
    "print(f\"inputs_tensor: {features_tensor.size()}\")\n",
    "features_tensor = features_tensor.T\n",
    "print(f\"inputs_tensor: {features_tensor.size()}\")\n",
    "targets_tensor = torch.Tensor(targets_binary)\n",
    "print(f\"inputs_clean len0 x len1: {len(features)} x {len(features[0])} -> inputs_tensor.shape: {features_tensor.shape}\")\n",
    "print(f\"targets_binary.shape: {len(targets_binary)} -> targets_tensor.shape: {targets_tensor.shape}\")\n",
    "print(f\"inputs_tensor: {features_tensor}\")\n",
    "print(f\"targets_tensor: {targets_tensor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features_tensor_test, targets_tensor_test= features_tensor, targets_tensor\n",
    "\n",
    "print(\"Test dataset frequencies:\")\n",
    "lu.display_frequency_values(targets_tensor_test.tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class StockDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        inputs,\n",
    "        targets):\n",
    "        \n",
    "        self.inputs= inputs\n",
    "        self.targets= targets\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, int]:\n",
    "        return self.inputs[index], self.targets[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# TODO: When executing only using 33-38% GPU - Try different BATCH_SIZE see if parallelism increases? Learning decreases because less batches?\n",
    "BATCH_SIZE= 32\n",
    "\n",
    "test_dataset= StockDataset(\n",
    "  features_tensor_test,\n",
    "  targets_tensor_test\n",
    ")\n",
    "\n",
    "print(f\"First input vector:\\n{test_dataset[0]}\")\n",
    "\n",
    "test_dataloader= DataLoader(\n",
    "  dataset=test_dataset,\n",
    "  batch_size=BATCH_SIZE,\n",
    "  shuffle=False\n",
    ")\n",
    "\n",
    "test_input0, test_target0= next(iter(test_dataloader))\n",
    "print(f\"Dataloader batch={BATCH_SIZE}\\nInput:\\n{test_input0}\\nTargets:\\n{test_target0}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXECUTE FROM THIS STEP To CREATE A NETWORK WITH RANDOM WEIGHTS\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class StockModelBinaryV0(nn.Module):\n",
    "  def __init__(self, input_features, hidden_units):\n",
    "    \"\"\"Initializes multi-class classification model\"\"\"\n",
    "    super().__init__()\n",
    "    self.linear_layer_stack = nn.Sequential(\n",
    "      nn.Linear(in_features=input_features, out_features=hidden_units*16),\n",
    "      nn.LeakyReLU(negative_slope=0.1),\n",
    "      nn.Linear(in_features=hidden_units*16, out_features=hidden_units*8),\n",
    "      nn.LeakyReLU(negative_slope=0.1),\n",
    "      nn.Linear(in_features=hidden_units*8, out_features=hidden_units*4),\n",
    "      nn.LeakyReLU(negative_slope=0.1),\n",
    "      nn.Linear(in_features=hidden_units*4, out_features=hidden_units),\n",
    "      nn.LeakyReLU(negative_slope=0.1),\n",
    "      nn.Linear(in_features=hidden_units, out_features=1)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    # print(\"forward x: \",\", \".join([str(num) for num in x.tolist()]))\n",
    "    # Layers are defined inside the Sequencial NN and will be applied here.\n",
    "    return self.linear_layer_stack(x)\n",
    "\n",
    "# Create an instance of the model\n",
    "model_0 = StockModelBinaryV0(\n",
    "  input_features=features_tensor.shape[1],\n",
    "  hidden_units=HIDDEN_UNITS).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads model from file\n",
    "from pathlib import Path\n",
    "\n",
    "# Create directory, if it doesn't exist, to store models\n",
    "MODEL_PATH= Path(\"models\")\n",
    "MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
    "\n",
    "model_0.load_state_dict(torch.load(f=MODEL_SAVE_PATH))\n",
    "model_0.to(device)\n",
    "\n",
    "print(f\"Test model loaded: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import ConfusionMatrix, Accuracy, Precision\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "\n",
    "model_0.eval()\n",
    "test_precision= 0\n",
    "with torch.inference_mode():\n",
    "    X= features_tensor_test.to(device)\n",
    "    y= targets_tensor_test.to(device)\n",
    "\n",
    "    # Predict for test data\n",
    "    test_logits= model_0(X).view(-1)\n",
    "    sigmoid_output = torch.sigmoid(test_logits)\n",
    "    test_pred = (sigmoid_output > TEST_THRESHOLD).float()    \n",
    "\n",
    "confmat= ConfusionMatrix(task='binary')\n",
    "\n",
    "# test_data.targets are the values we want to predict in the test dataloader\n",
    "confmat_tensor= confmat(\n",
    "  preds= test_pred.cpu(),\n",
    "  target= targets_tensor_test.cpu())\n",
    "\n",
    "# Plot confusion matrix\n",
    "fig, ax= plot_confusion_matrix(\n",
    "  conf_mat= confmat_tensor.numpy(),\n",
    "  figsize= (10, 7)\n",
    ")\n",
    "\n",
    "accuracy_fn= Accuracy(task='binary').to(device)\n",
    "test_accuracy = accuracy_fn(test_pred, y)\n",
    "print(f\"Test threshold: {TEST_THRESHOLD}\")\n",
    "print(f\"Test confusion matrix:\\n{confmat_tensor}\")\n",
    "\n",
    "precision_fn= Precision(task='binary').to(device)\n",
    "test_precision = precision_fn(test_pred, y)\n",
    "print(f\"Test Accuracy: {test_accuracy*100:.2f}%\")\n",
    "print(f\"Test Precision: {test_precision*100:.2f}%\")\n",
    "false_positives = confmat_tensor[0, 1].item()\n",
    "true_positives = confmat_tensor[1, 1].item()\n",
    "print(f\"Test false_positives: {false_positives} true_positives: {true_positives}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of threshold adjustment after model training\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import numpy as np\n",
    "\n",
    "model_0.eval()\n",
    "with torch.no_grad():\n",
    "    X= features_tensor_test.to(device)\n",
    "    y= targets_tensor_test.to(device)\n",
    "\n",
    "    logits = model_0(X)\n",
    "    probs = torch.sigmoid(logits).cpu().numpy()\n",
    "    y_cpu = y.cpu().numpy()\n",
    "    \n",
    "precision, recall, thresholds = precision_recall_curve(y_cpu, probs)\n",
    "\n",
    "# Find the threshold that gives the highest precision\n",
    "optimal_idx = np.argmax(precision[:-1])\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "print(\"Optimal threshold: \", optimal_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "#### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"======TEST:\")\n",
    "print(f\"Trained model: {MODEL_NAME}\")\n",
    "print(f\"Ticker: {TICKERS}\")\n",
    "\n",
    "print(f\"Data start: {new_data[0]['date'][0]} end: {new_data[0]['date'].iloc[-1]}\")\n",
    "print(f\"Data Interval: {DATA_INTERVAL_MINUTES} - After Hours: {DATA_AFTER_HOURS}\")\n",
    "\n",
    "print(f\"EWA Reach: {REACH_PCT}\")\n",
    "print(f\"Price Averages: {PRICE_AVGS}\")\n",
    "print(f\"points_features_extractor: {points_features_extractor.__class__.__name__}\")\n",
    "print(f\"points_target_extractor: {points_target_extractor.__class__.__name__}\")\n",
    "print(f\"Predict {'UP' if PREDICT_UP else 'DOWN'} - days: {DAYS_PREDICT} Down pcts: {DOWN_PCTS_PREDICT} Up pcts: {UP_PCTS_PREDICT}\")\n",
    "\n",
    "print(\"Targets Frequencies:\")\n",
    "lu.display_frequency_classes(targets, DOWN_PCTS_PREDICT, UP_PCTS_PREDICT)\n",
    "\n",
    "print(\"Test dataset frequencies:\")\n",
    "lu.display_frequency_values(targets_tensor_test.tolist())\n",
    "\n",
    "print(\"--Inference\")\n",
    "print(f\"Network hidden units: {HIDDEN_UNITS}\")\n",
    "print(f\"== Test threshold: {TEST_THRESHOLD}\")\n",
    "\n",
    "print(\"--Inference Results\")\n",
    "print(f\"Test confusion matrix:\\n{confmat_tensor}\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")\n",
    "print(f\"Test precision: {test_precision*100:.2f}%\")\n",
    "print(f\"Test false_positives: {false_positives} true_positives: {true_positives}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "predict-backtest",
   "language": "python",
   "name": "predict-backtest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
