{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Model Test\n",
    "Uses a pre-trained model to make predictions for dates that occur after the last date included in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "\n",
    "import sys\n",
    "sys.path.append('..\\\\..')\n",
    "print(sys.path)\n",
    "\n",
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### SET PARAMETERS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.text_utils as tu\n",
    "\n",
    "MODEL_NAME= f\"2024-06-01-0656-TSLA-predictDOWN-dates20190102-20231229-days3-down300-up300-in16-hid12-pos_weight13979-prec9737pct-fp22tp815-high_prec.pth\"\n",
    "\n",
    "TICKER= tu.extract_company(MODEL_NAME)\n",
    "DATA_INTERVAL_MINUTES = 15   # (Set to 5 or 15)\n",
    "DATA_AFTER_HOURS = False\n",
    "\n",
    "DAYS_PREDICT = tu.extract_days(MODEL_NAME)\n",
    "DOWN_PCTS_PREDICT= tu.extract_pcts(MODEL_NAME, \"down\")\n",
    "UP_PCTS_PREDICT= tu.extract_pcts(MODEL_NAME, \"up\")\n",
    "\n",
    "PREDICT_UP = tu.is_predict_up(MODEL_NAME)\n",
    "if PREDICT_UP:\n",
    "    INDEX_KEEP= 2\n",
    "    INDEX_REMOVE_A= 0\n",
    "    INDEX_REMOVE_B= 1\n",
    "else:\n",
    "    INDEX_KEEP= 0\n",
    "    INDEX_REMOVE_A= 1\n",
    "    INDEX_REMOVE_B= 2\n",
    "\n",
    "signal_avg= [\n",
    "    2, \n",
    "    3, \n",
    "    5, \n",
    "    8, \n",
    "    13, \n",
    "    21, \n",
    "    34, \n",
    "    55, \n",
    "    89, \n",
    "    144, \n",
    "    233, \n",
    "    377, \n",
    "    610, \n",
    "    987, \n",
    "    1597, \n",
    "    2584]\n",
    "\n",
    "HIDDEN_UNITS=12\n",
    "\n",
    "TEST_THRESHOLD = 0.9598954\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "# Load training data (!!!ONLY TO VERIFY):\n",
    "Use to verify results: compare with results obtain when training\n",
    "If don't need to verify training data Skip to step that load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import settings\n",
    "import apis.tiingo_api as tiingo\n",
    "\n",
    "secret_key= settings.get_secret(\"tiingo-key\")\n",
    "client = tiingo.TiingoAPI(secret_key)\n",
    "\n",
    "\n",
    "### TRAINING DATA\n",
    "csv_data2019= client.download_ticker(TICKER, datetime(2019, 1, 1), datetime(2020,1,1), DATA_INTERVAL_MINUTES, DATA_AFTER_HOURS)\n",
    "csv_data2020= client.download_ticker(TICKER, datetime(2020, 1, 1), datetime(2021,1,1), DATA_INTERVAL_MINUTES, DATA_AFTER_HOURS)\n",
    "csv_data2021= client.download_ticker(TICKER, datetime(2021, 1, 1), datetime(2022,1,1), DATA_INTERVAL_MINUTES, DATA_AFTER_HOURS)\n",
    "csv_data2022= client.download_ticker(TICKER, datetime(2022, 1, 1), datetime(2023,1,1), DATA_INTERVAL_MINUTES, DATA_AFTER_HOURS)\n",
    "csv_data2023= client.download_ticker(TICKER, datetime(2023, 1, 1), datetime(2024,1,1), DATA_INTERVAL_MINUTES, DATA_AFTER_HOURS)\n",
    "# csv_data2024= tiingo.download_ticker(secret_key, TICKER, datetime(2024, 1, 1), datetime(2024,2,1), DATA_INTERVAL_MINUTES, DATA_AFTER_HOURS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import pandas as pd\n",
    "\n",
    "# ### TRAINING DATA\n",
    "df2019 = pd.read_csv(io.StringIO(csv_data2019))\n",
    "df2020 = pd.read_csv(io.StringIO(csv_data2020))\n",
    "df2021 = pd.read_csv(io.StringIO(csv_data2021))\n",
    "df2022 = pd.read_csv(io.StringIO(csv_data2022))\n",
    "df2023 = pd.read_csv(io.StringIO(csv_data2023))\n",
    "# df2024 = pd.read_csv(io.StringIO(csv_data2024))\n",
    "\n",
    "if not df2019.empty:\n",
    "    print(\"Concatenating from 2019\")\n",
    "    df = pd.concat([df2019, df2020, df2021, df2022, df2023], axis=0, ignore_index=True)\n",
    "elif not df2020.empty:\n",
    "    print(\"Concatenating from 2020\")\n",
    "    df = pd.concat([df2020, df2021, df2022, df2023], axis=0, ignore_index=True)\n",
    "else:\n",
    "    print(\"Concatenating from 2021\")\n",
    "    df = pd.concat([df2021, df2022, df2023], axis=0, ignore_index=True)\n",
    "\n",
    "# if not df2024.empty:\n",
    "#     print(\"Concatenating from 2024\")\n",
    "#     df = pd.concat([df, df2024], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## LOADING TEST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import settings\n",
    "import apis.tiingo_api as tiingo\n",
    "\n",
    "secret_key= settings.get_secret(\"tiingo-key\")\n",
    "client = tiingo.TiingoAPI(secret_key)\n",
    "\n",
    "# # For first prediction need signal_avg[-1]=2584 ticks -> 2584 / 26 ~ 100 days -> 100 * 7 / 5 / 30 = 4.6 months\n",
    "csv_data2023= client.download_ticker(TICKER, datetime(2023, 8, 1), datetime(2023,12,31), DATA_INTERVAL_MINUTES, DATA_AFTER_HOURS)\n",
    "csv_data2024= client.download_ticker(TICKER, datetime(2024, 1, 1), datetime(2024,5,25), DATA_INTERVAL_MINUTES, DATA_AFTER_HOURS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import pandas as pd\n",
    "\n",
    "### TEST PREDICTIONS ON RECENT DATA\n",
    "df2023 = pd.read_csv(io.StringIO(csv_data2023))\n",
    "df2024 = pd.read_csv(io.StringIO(csv_data2024))\n",
    "df = pd.concat([df2023, df2024], axis=0, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validates that data has been concatenated correctly = ordered ascending\n",
    "if df[\"date\"].is_monotonic_increasing and df[\"date\"].is_unique:\n",
    "    print(\"Correct: DataFrame is in ascending order.\")\n",
    "else:\n",
    "    print(\"Error: DataFrame is not in ascending order.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify expected dates correspond with the data you intend to use\n",
    "print(f\"Test data first:\\n{df[['date', 'close']][:5]}\")\n",
    "print(f\"Test data last:\\n{df[['date', 'close']][-5:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If quotes every 15min there 26 per day if quotes every 5min there are 78 per day\n",
    "TICKS_IN_DAY = 26 if DATA_INTERVAL_MINUTES == 15 else 78\n",
    "# How many data ticks are inspecting to determine the if up or down by percentage \n",
    "TICKS_PREDICT= TICKS_IN_DAY * DAYS_PREDICT\n",
    "REACH_PCT= 0.95\n",
    "\n",
    "import classifiers.up_down_classifier as udc\n",
    "import classifiers.ewa_classifier as ec\n",
    "\n",
    "alpha= ec.calculate_ewa_alpha(TICKS_PREDICT, REACH_PCT)\n",
    "print(f\"alpha: {alpha:.4f} for window: {TICKS_PREDICT} and reach: {REACH_PCT}\")\n",
    "\n",
    "classes_calc = udc.UpsDownsClassifier(TICKS_PREDICT, DOWN_PCTS_PREDICT, UP_PCTS_PREDICT)\n",
    "\n",
    "close_prices = df['close'].astype(float).tolist()\n",
    "input_data= ec.calculate_ewas(close_prices, alpha)\n",
    "\n",
    "classes= classes_calc.classify(input_data)\n",
    "print(f\"Check correct '-1' point (window={TICKS_PREDICT}): {classes[-TICKS_PREDICT-1:-TICKS_PREDICT+1]}\")\n",
    "print(f\"prices vs input_data: {[(p, c) for p, c in zip(close_prices[2650:3000], input_data[2650:3000])]}\")\n",
    "print(f\"input_data vs classes: {[(p, c) for p, c in zip(input_data[2650:3000], classes[2650:3000])]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display histogram for the classes\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a histogram\n",
    "hist_values, bin_edges, _ = plt.hist(classes, bins=4, edgecolor='black')\n",
    "\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Data')\n",
    "\n",
    "# Display frequency on top of each bar\n",
    "for value, edge in zip(hist_values, bin_edges[:-1]):\n",
    "    plt.text(float(edge), float(value), str(int(value)), color='black')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show percentages of each class value\n",
    "import utils.list_utils as lu\n",
    "\n",
    "lu.display_frequency_classes(classes, DOWN_PCTS_PREDICT, UP_PCTS_PREDICT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Display classes value changes over time (last 500 ticks)\n",
    "graph_ticks = 500\n",
    "x = range(len(classes[-graph_ticks:]))\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(x, classes[-graph_ticks:], linestyle='-')\n",
    "\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Class')\n",
    "plt.title('Plot of Classes')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate the signals as input for the neural network as proportions\n",
    "import preprocessing.proportions_calc as proportions\n",
    "\n",
    "signals_calculator = proportions.ProportionsCalc(signal_avg)\n",
    "\n",
    "proportions_avg = signals_calculator.calculate(close_prices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Prices length: {len(close_prices)}\")\n",
    "print(f\"Proportions length: {len(proportions_avg[-1])}\")\n",
    "\n",
    "print(f\"Last 10 close: {close_prices[-10:]}\")\n",
    "print(f\"Last 10 proportions(avg={signal_avg[0]}): {proportions_avg[0][-10:]}\")\n",
    "\n",
    "print(f\"Proportions avgs: Length: {len(signal_avg)} Last: {signal_avg[-1]}\")\n",
    "# At the end of the data, when less ticks than necessary no possible to predict so \"-1\" \n",
    "print(f\"Classes last non-negative-1: {classes[-TICKS_PREDICT-1:-TICKS_PREDICT+1]} len: {len(classes)}\")\n",
    "print(f\"Proportions first non-negative-1(avg={signal_avg[-1]}): {proportions_avg[-1][signal_avg[-1]-2:signal_avg[-1]]} len: {len(proportions_avg[-1])}\")\n",
    "print(f\"Proportions (avg={signal_avg[0]}) Min: {min(proportions_avg[0][signal_avg[0]-1:-TICKS_PREDICT-1])} Max: {max(proportions_avg[0][signal_avg[0]-1:-TICKS_PREDICT])}\")\n",
    "print(f\"Proportions (avg={signal_avg[-1]}) Min: {min(proportions_avg[-1][signal_avg[-1]-1:-TICKS_PREDICT-1])} Max: {max(proportions_avg[-1][signal_avg[-1]-1:-TICKS_PREDICT])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the \"-1\" from the proportions\n",
    "#   At the beging first signal_avg[-1] are \"-1\" (need previous values for first avg.)\n",
    "#   At the end decided not predict if period to predict is shorter\n",
    "targets = classes[signal_avg[-1]-1:-TICKS_PREDICT]\n",
    "inputs = []\n",
    "for proportion in proportions_avg:\n",
    "    proportion_cut= proportion[signal_avg[-1]-1:-TICKS_PREDICT]\n",
    "    print(proportion_cut[:2])\n",
    "    inputs.append(proportion_cut)\n",
    "\n",
    "print(f\"First target: {targets[0]} and last target: {targets[-1]}\")\n",
    "print(f\"Classes: {len(classes)} after cut to targets: {len(targets)}\")\n",
    "print(f\"Inputs len: {len(inputs[len(signal_avg)-1])}\")\n",
    "print(f\"Distinct targets: {list(set(targets))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print(\"Test data: '-1' removed from begining and end\")\n",
    "lu.display_frequency_classes(targets, DOWN_PCTS_PREDICT, UP_PCTS_PREDICT)\n",
    "\n",
    "targets_frequency = Counter(targets)\n",
    "print(\"VALIDATE removing should be POSITIVE?\")\n",
    "count_remove_a= targets_frequency[INDEX_REMOVE_A] - targets_frequency[INDEX_KEEP] + targets_frequency[INDEX_KEEP] //2\n",
    "count_remove_b= targets_frequency[INDEX_REMOVE_B] - targets_frequency[INDEX_KEEP] + targets_frequency[INDEX_KEEP] //2\n",
    "print(f\"Removing {INDEX_REMOVE_A}: {count_remove_a}\")\n",
    "print(f\"Removing {INDEX_REMOVE_B}: {count_remove_b}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "##### SET PARAMETERS\n",
    "###################\n",
    "# DECISION SET: REMOVING?\n",
    "indexes_remove_a= []\n",
    "# 2024-03-01 Do not remove anything\n",
    "# if count_remove_a > 0:\n",
    "#     indexes_remove_a = get_indexes_value(targets, index_remove_a, count_remove_a)\n",
    "\n",
    "# DECISION SET: REMOVING?\n",
    "indexes_remove_b= []\n",
    "# 2024-03-01 Do not remove anything\n",
    "# if count_remove_b > 0:\n",
    "#     indexes_remove_b = get_indexes_value(targets, index_remove_b, count_remove_b)\n",
    "\n",
    "indexes_remove= indexes_remove_a + indexes_remove_b\n",
    "targets_clean= lu.remove_indexes(targets, indexes_remove)\n",
    "\n",
    "lu.display_frequency_classes(targets_clean, DOWN_PCTS_PREDICT, UP_PCTS_PREDICT)\n",
    "print(f\"Targets len: {len(targets)} Targets clean: {len(targets_clean)} Difference: {len(targets)-len(targets_clean)}\")\n",
    "\n",
    "inputs_clean = [lu.remove_indexes(input, indexes_remove) for input in inputs]    \n",
    "print(f\"targets_clean positions(Keep={INDEX_KEEP})(First:{targets_clean.index(INDEX_KEEP)},Last:-{targets_clean[::-1].index(INDEX_KEEP)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets 'index_keep' as target = 1 and rest of indexes to target=0\n",
    "targets_binary= lu.convert_binary(targets_clean, INDEX_KEEP)\n",
    "print(f\"targets_binary First {targets_binary.index(True)} and Last(counting from end) {targets_binary[::-1].index(True)} position with True\")\n",
    "print(f\"targets_binary len: {len(targets_binary)} Input clean[0]: {len(inputs_clean[0])} Input clean[-1]: {len(inputs_clean[-1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "inputs_tensor = torch.Tensor(inputs_clean)\n",
    "print(f\"inputs_tensor: {inputs_tensor.size()}\")\n",
    "inputs_tensor = inputs_tensor.T\n",
    "print(f\"inputs_tensor: {inputs_tensor.size()}\")\n",
    "targets_tensor = torch.Tensor(targets_binary)\n",
    "print(f\"inputs_clean len0 x len1: {len(inputs_clean)} x {len(inputs_clean[0])} -> inputs_tensor.shape: {inputs_tensor.shape}\")\n",
    "print(f\"targets_binary.shape: {len(targets_binary)} -> targets_tensor.shape: {targets_tensor.shape}\")\n",
    "print(f\"inputs_tensor: {inputs_tensor}\")\n",
    "print(f\"targets_tensor: {targets_tensor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inputs_tensor_test, targets_tensor_test= inputs_tensor, targets_tensor\n",
    "\n",
    "print(\"Test dataset frequencies:\")\n",
    "lu.display_frequency_values(targets_tensor_test.tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class StockDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        inputs,\n",
    "        targets):\n",
    "        \n",
    "        self.inputs= inputs\n",
    "        self.targets= targets\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, int]:\n",
    "        return self.inputs[index], self.targets[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# TODO: When executing only using 33-38% GPU - Try different BATCH_SIZE see if parallelism increases? Learning decreases because less batches?\n",
    "BATCH_SIZE= 32\n",
    "\n",
    "test_dataset= StockDataset(\n",
    "  inputs_tensor_test,\n",
    "  targets_tensor_test\n",
    ")\n",
    "\n",
    "print(f\"First input vector:\\n{test_dataset[0]}\")\n",
    "\n",
    "test_dataloader= DataLoader(\n",
    "  dataset=test_dataset,\n",
    "  batch_size=BATCH_SIZE,\n",
    "  shuffle=False\n",
    ")\n",
    "\n",
    "test_input0, test_target0= next(iter(test_dataloader))\n",
    "print(f\"Dataloader batch={BATCH_SIZE}\\nInput:\\n{test_input0}\\nTargets:\\n{test_target0}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXECUTE FROM THIS STEP To CREATE A NETWORK WITH RANDOM WEIGHTS\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class StockModelBinaryV0(nn.Module):\n",
    "  def __init__(self, input_features, hidden_units):\n",
    "    \"\"\"Initializes multi-class classification model\"\"\"\n",
    "    super().__init__()\n",
    "    self.linear_layer_stack = nn.Sequential(\n",
    "      nn.Linear(in_features=input_features, out_features=hidden_units*16),\n",
    "      nn.LeakyReLU(negative_slope=0.1),\n",
    "      nn.Linear(in_features=hidden_units*16, out_features=hidden_units*8),\n",
    "      nn.LeakyReLU(negative_slope=0.1),\n",
    "      nn.Linear(in_features=hidden_units*8, out_features=hidden_units*4),\n",
    "      nn.LeakyReLU(negative_slope=0.1),\n",
    "      nn.Linear(in_features=hidden_units*4, out_features=hidden_units),\n",
    "      nn.LeakyReLU(negative_slope=0.1),\n",
    "      nn.Linear(in_features=hidden_units, out_features=1)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    # print(\"forward x: \",\", \".join([str(num) for num in x.tolist()]))\n",
    "    # Layers are defined inside the Sequencial NN and will be applied here.\n",
    "    return self.linear_layer_stack(x)\n",
    "\n",
    "# Create an instance of the model\n",
    "model_0 = StockModelBinaryV0(\n",
    "  input_features=len(signal_avg),\n",
    "  hidden_units=HIDDEN_UNITS).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads model from file\n",
    "from pathlib import Path\n",
    "\n",
    "# Create directory, if it doesn't exist, to store models\n",
    "MODEL_PATH= Path(\"models\")\n",
    "MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
    "\n",
    "model_0.load_state_dict(torch.load(f=MODEL_SAVE_PATH))\n",
    "model_0.to(device)\n",
    "\n",
    "print(f\"Test model loaded: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import ConfusionMatrix, Accuracy, Precision\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "\n",
    "model_0.eval()\n",
    "test_precision= 0\n",
    "with torch.inference_mode():\n",
    "    X= inputs_tensor_test.to(device)\n",
    "    y= targets_tensor_test.to(device)\n",
    "\n",
    "    # Predict for test data\n",
    "    test_logits= model_0(X).view(-1)\n",
    "    sigmoid_output = torch.sigmoid(test_logits)\n",
    "    test_pred = (sigmoid_output > TEST_THRESHOLD).float()    \n",
    "\n",
    "confmat= ConfusionMatrix(task='binary')\n",
    "\n",
    "# test_data.targets are the values we want to predict in the test dataloader\n",
    "confmat_tensor= confmat(\n",
    "  preds= test_pred.cpu(),\n",
    "  target= targets_tensor_test.cpu())\n",
    "\n",
    "# Plot confusion matrix\n",
    "fig, ax= plot_confusion_matrix(\n",
    "  conf_mat= confmat_tensor.numpy(),\n",
    "  figsize= (10, 7)\n",
    ")\n",
    "\n",
    "accuracy_fn= Accuracy(task='binary').to(device)\n",
    "test_accuracy = accuracy_fn(test_pred, y)\n",
    "print(f\"Test threshold: {TEST_THRESHOLD}\")\n",
    "print(f\"Test confusion matrix:\\n{confmat_tensor}\")\n",
    "\n",
    "precision_fn= Precision(task='binary').to(device)\n",
    "test_precision = precision_fn(test_pred, y)\n",
    "print(f\"Test Accuracy: {test_accuracy*100:.2f}%\")\n",
    "print(f\"Test Precision: {test_precision*100:.2f}%\")\n",
    "false_positives = confmat_tensor[0, 1].item()\n",
    "true_positives = confmat_tensor[1, 1].item()\n",
    "print(f\"Test false_positives: {false_positives} true_positives: {true_positives}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of threshold adjustment after model training\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import numpy as np\n",
    "\n",
    "model_0.eval()\n",
    "with torch.no_grad():\n",
    "    X= inputs_tensor_test.to(device)\n",
    "    y= targets_tensor_test.to(device)\n",
    "\n",
    "    logits = model_0(X)\n",
    "    probs = torch.sigmoid(logits).cpu().numpy()\n",
    "    y_cpu = y.cpu().numpy()\n",
    "    \n",
    "precision, recall, thresholds = precision_recall_curve(y_cpu, probs)\n",
    "\n",
    "# Find the threshold that gives the highest precision\n",
    "optimal_idx = np.argmax(precision[:-1])\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "print(\"Optimal threshold: \", optimal_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "#### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"======TEST:\")\n",
    "print(f\"Trained model: {MODEL_NAME}\")\n",
    "print(f\"Ticker: {TICKER}\")\n",
    "\n",
    "print(f\"Data start: {df['date'][0]} end: {df['date'].iloc[-1]}\")\n",
    "print(f\"Data Interval: {DATA_INTERVAL_MINUTES} - After Hours: {DATA_AFTER_HOURS}\")\n",
    "\n",
    "print(f\"Signal Averages: {signal_avg}\")\n",
    "print(f\"Predict {'UP' if PREDICT_UP else 'DOWN'} - days: {DAYS_PREDICT} Down pcts: {DOWN_PCTS_PREDICT} Up pcts: {UP_PCTS_PREDICT}\")\n",
    "print(\"Targets Frequencies:\")\n",
    "lu.display_frequency_classes(targets_clean, DOWN_PCTS_PREDICT, UP_PCTS_PREDICT)\n",
    "\n",
    "print(\"Test dataset frequencies:\")\n",
    "lu.display_frequency_values(targets_tensor_test.tolist())\n",
    "\n",
    "print(\"--Inference\")\n",
    "print(f\"Network hidden units: {HIDDEN_UNITS}\")\n",
    "print(f\"== Test threshold: {TEST_THRESHOLD}\")\n",
    "\n",
    "print(\"--Inference Results\")\n",
    "print(f\"Test confusion matrix:\\n{confmat_tensor}\")\n",
    "print(f\"Val accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Val precision: {val_precision*100:.2f}%\")\n",
    "print(f\"Val false_positives: {false_positives} true_positives: {true_positives}\")\n",
    "print(f\"Val Precision manual: {precision_m*100:.2f}%\")\n",
    "print(f\"Val Recall manual: {recall_m*100:.2f}%\")\n",
    "print(f\"Val F1-score manual: {2 * precision_m * recall_m / (precision_m + recall_m):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
