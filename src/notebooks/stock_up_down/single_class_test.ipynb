{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f360a21f",
   "metadata": {},
   "source": [
    "# Model Test\n",
    "Uses a pre-trained model to make predictions for dates that occur after the last date included in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea3d93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d154e01b-69f9-4b2e-adf2-34a7b8833a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fd91ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "##### SET PARAMETERS\n",
    "################### \n",
    "TICKER=\"TSLA\"\n",
    "DATA_INTERVAL_MINUTES = 15   # (Set to 5 or 15)\n",
    "DATA_AFTER_HOURS = False\n",
    "###################L\n",
    "##### SET PARAMETERS\n",
    "###################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b171f60",
   "metadata": {},
   "source": [
    "# Load training data:\n",
    "Use to verify results: compare with results obtain when training\n",
    "If don't need to verify training data Skip to step that load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c37e701",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..\\\\..')\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import settings\n",
    "import apis.tiingo_api as tiingo\n",
    "\n",
    "secret_key= settings.get_secret(\"tiingo-key\")\n",
    "\n",
    "### TRAINING DATA\n",
    "csv_data2019= tiingo.download_ticker(secret_key, TICKER, datetime(2019, 1, 1), datetime(2020,1,1), DATA_INTERVAL_MINUTES, DATA_AFTER_HOURS)\n",
    "csv_data2020= tiingo.download_ticker(secret_key, TICKER, datetime(2020, 1, 1), datetime(2021,1,1), DATA_INTERVAL_MINUTES, DATA_AFTER_HOURS)\n",
    "csv_data2021= tiingo.download_ticker(secret_key, TICKER, datetime(2021, 1, 1), datetime(2022,1,1), DATA_INTERVAL_MINUTES, DATA_AFTER_HOURS)\n",
    "csv_data2022= tiingo.download_ticker(secret_key, TICKER, datetime(2022, 1, 1), datetime(2023,1,1), DATA_INTERVAL_MINUTES, DATA_AFTER_HOURS)\n",
    "csv_data2023= tiingo.download_ticker(secret_key, TICKER, datetime(2023, 1, 1), datetime(2024,1,1), DATA_INTERVAL_MINUTES, DATA_AFTER_HOURS)\n",
    "# csv_data2024= tiingo.download_ticker(secret_key, TICKER, datetime(2024, 1, 1), datetime(2024,2,1), DATA_INTERVAL_MINUTES, DATA_AFTER_HOURS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40af303f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import pandas as pd\n",
    "\n",
    "# ### TRAINING DATA\n",
    "df2019 = pd.read_csv(io.StringIO(csv_data2019))\n",
    "df2020 = pd.read_csv(io.StringIO(csv_data2020))\n",
    "df2021 = pd.read_csv(io.StringIO(csv_data2021))\n",
    "df2022 = pd.read_csv(io.StringIO(csv_data2022))\n",
    "df2023 = pd.read_csv(io.StringIO(csv_data2023))\n",
    "# df2024 = pd.read_csv(io.StringIO(csv_data2024))\n",
    "\n",
    "if not df2019.empty:\n",
    "    print(\"Concatenating from 2019\")\n",
    "    df = pd.concat([df2019, df2020, df2021, df2022, df2023], axis=0, ignore_index=True)\n",
    "elif not df2020.empty:\n",
    "    print(\"Concatenating from 2020\")\n",
    "    df = pd.concat([df2020, df2021, df2022, df2023], axis=0, ignore_index=True)\n",
    "else:\n",
    "    print(\"Concatenating from 2021\")\n",
    "    df = pd.concat([df2021, df2022, df2023], axis=0, ignore_index=True)\n",
    "\n",
    "# if not df2024.empty:\n",
    "#     print(\"Concatenating from 2024\")\n",
    "#     df = pd.concat([df, df2024], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f7740c",
   "metadata": {},
   "source": [
    "## LOADING TEST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62afeeba-6a9c-41cf-bbf7-da672fc6ff5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..\\\\..')\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import settings\n",
    "import apis.tiingo_api as tiingo\n",
    "\n",
    "secret_key= settings.get_secret(\"tiingo-key\")\n",
    "\n",
    "# # For first prediction need signal_avg[-1]=2584 ticks -> 2584 / 26 ~ 100 days -> 100 * 7 / 5 / 30 = 4.6 months\n",
    "csv_data2023= tiingo.download_ticker(secret_key, TICKER, datetime(2023, 8, 1), datetime(2023,12,31), DATA_INTERVAL_MINUTES, DATA_AFTER_HOURS)\n",
    "csv_data2024= tiingo.download_ticker(secret_key, TICKER, datetime(2024, 1, 1), datetime(2024,5,3), DATA_INTERVAL_MINUTES, DATA_AFTER_HOURS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a77e70-8b1d-4be0-9330-cfe1fe98ddaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import pandas as pd\n",
    "\n",
    "### TEST PREDICTIONS ON RECENT DATA\n",
    "df2023 = pd.read_csv(io.StringIO(csv_data2023))\n",
    "df2024 = pd.read_csv(io.StringIO(csv_data2024))\n",
    "df = pd.concat([df2023, df2024], axis=0, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66b7cc7-98a6-43a0-82b1-0b2abdc83d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validates that data has been concatenated correctly = ordered ascending\n",
    "if df[\"date\"].is_monotonic_increasing and df[\"date\"].is_unique:\n",
    "    print(\"Correct: DataFrame is in ascending order.\")\n",
    "else:\n",
    "    print(\"Error: DataFrame is not in ascending order.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de91e28-bd1d-41bf-899b-4e183ae5be45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify expected dates correspond with the data you intend to use\n",
    "print(f\"Test data first:\\n{df[['date', 'close']][:5]}\")\n",
    "print(f\"Test data last:\\n{df[['date', 'close']][-5:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c802c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "##### SET PARAMETERS\n",
    "###################\n",
    "DAYS_PREDICT = 5\n",
    "DOWN_PCTS_PREDICT= [2.]\n",
    "UP_PCTS_PREDICT= [5.]\n",
    "###################\n",
    "##### SET PARAMETERS\n",
    "###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e990fa6-00fc-496d-be86-955866c3ffb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If quotes every 15min there 26 per day if quotes every 5min there are 78 per day\n",
    "TICKS_IN_DAY = 26 if DATA_INTERVAL_MINUTES == 15 else 78\n",
    "# How many data ticks are inspecting to determine the if up or down by percentage \n",
    "TICKS_PREDICT= TICKS_IN_DAY * DAYS_PREDICT\n",
    "REACH_PCT= 0.95\n",
    "\n",
    "import classifiers.up_down_classifier as udc\n",
    "import classifiers.ewa_classifier as ec\n",
    "\n",
    "alpha= ec.calculate_ewa_alpha(TICKS_PREDICT, REACH_PCT)\n",
    "print(f\"alpha: {alpha:.4f} for window: {TICKS_PREDICT} and reach: {REACH_PCT}\")\n",
    "\n",
    "classes_calc = udc.UpsDownsClassifier(TICKS_PREDICT, DOWN_PCTS_PREDICT, UP_PCTS_PREDICT)\n",
    "\n",
    "close_prices = df['close'].astype(float).tolist()\n",
    "input_data= ec.calculate_ewas(close_prices, alpha)\n",
    "\n",
    "classes= classes_calc.classify(input_data)\n",
    "print(f\"Check correct 'nan' point (window={TICKS_PREDICT}): {classes[-TICKS_PREDICT-1:-TICKS_PREDICT+1]}\")\n",
    "print(f\"prices vs input_data: {[(p, c) for p, c in zip(close_prices[2650:3000], input_data[2650:3000])]}\")\n",
    "print(f\"input_data vs classes: {[(p, c) for p, c in zip(input_data[2650:3000], classes[2650:3000])]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a25bdb-a1ac-49b9-b508-55aa08e54c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display histogram for the classes\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a histogram\n",
    "hist_values, bin_edges, _ = plt.hist(classes, bins=21, edgecolor='black')\n",
    "\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Data')\n",
    "\n",
    "# Display frequency on top of each bar\n",
    "for value, edge in zip(hist_values, bin_edges[:-1]):\n",
    "    plt.text(edge, value, str(int(value)), color='black')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e25cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show percentages of each class value\n",
    "import utils.list_utils as lu\n",
    "\n",
    "lu.display_frequency_classes(classes, DOWN_PCTS_PREDICT, UP_PCTS_PREDICT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8ce196",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Display classes value changes over time (last 500 ticks)\n",
    "graph_ticks = 500\n",
    "x = range(len(classes[-graph_ticks:]))\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(x, classes[-graph_ticks:], linestyle='-')\n",
    "\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Class')\n",
    "plt.title('Plot of Classes')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e25c538",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "##### SET PARAMETERS\n",
    "###################\n",
    "signal_avg= [\n",
    "    2, \n",
    "    3, \n",
    "    5, \n",
    "    8, \n",
    "    13, \n",
    "    21, \n",
    "    34, \n",
    "    55, \n",
    "    89, \n",
    "    144, \n",
    "    233, \n",
    "    377, \n",
    "    610, \n",
    "    987, \n",
    "    1597, \n",
    "    2584]\n",
    "###################\n",
    "##### SET PARAMETERS\n",
    "###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1347910b-5c73-4c91-847e-906b66ad4e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate the signals as input for the neural network as proportions\n",
    "import preprocessing.proportions_calc as proportions\n",
    "\n",
    "signals_calculator = proportions.ProportionsCalc(signal_avg)\n",
    "\n",
    "proportions_avg = signals_calculator.calculate(close_prices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ac18bb-6cb7-4307-ab28-e8b5d5f2a74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Prices length: {len(close_prices)}\")\n",
    "print(f\"Proportions length: {len(proportions_avg[-1])}\")\n",
    "\n",
    "print(f\"Last 10 close: {close_prices[-10:]}\")\n",
    "print(f\"Last 10 proportions(avg={signal_avg[0]}): {proportions_avg[0][-10:]}\")\n",
    "\n",
    "print(f\"Proportions avgs: Length: {len(signal_avg)} Last: {signal_avg[-1]}\")\n",
    "# At the end of the data, when less ticks than necessary no possible to predict so \"nan\" \n",
    "print(f\"Classes last non-nan: {classes[-TICKS_PREDICT-1:-TICKS_PREDICT+1]} len: {len(classes)}\")\n",
    "print(f\"Proportions first non-nan(avg={signal_avg[-1]}): {proportions_avg[-1][signal_avg[-1]-2:signal_avg[-1]]} len: {len(proportions_avg[-1])}\")\n",
    "print(f\"Proportions (avg={signal_avg[0]}) Min: {min(proportions_avg[0][signal_avg[0]-1:-TICKS_PREDICT-1])} Max: {max(proportions_avg[0][signal_avg[0]-1:-TICKS_PREDICT])}\")\n",
    "print(f\"Proportions (avg={signal_avg[-1]}) Min: {min(proportions_avg[-1][signal_avg[-1]-1:-TICKS_PREDICT-1])} Max: {max(proportions_avg[-1][signal_avg[-1]-1:-TICKS_PREDICT])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bb2e59-3b6b-4691-8d9e-a5b0754c6f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the \"nan\" from the proportions\n",
    "#   At the beging first signal_avg[-1] are \"nan\" (need previous values for first avg.)\n",
    "#   At the end decided not predict if period to predict is shorter\n",
    "targets = classes[signal_avg[-1]-1:-TICKS_PREDICT]\n",
    "inputs = []\n",
    "for proportion in proportions_avg:\n",
    "    proportion_cut= proportion[signal_avg[-1]-1:-TICKS_PREDICT]\n",
    "    print(proportion_cut[:2])\n",
    "    inputs.append(proportion_cut)\n",
    "\n",
    "print(f\"First target: {targets[0]} and last target: {targets[-1]}\")\n",
    "print(f\"Classes: {len(classes)} after cut to targets: {len(targets)}\")\n",
    "print(f\"Inputs len: {len(inputs[len(signal_avg)-1])}\")\n",
    "print(f\"Distinct targets: {list(set(targets))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57aeff26",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "##### SET PARAMETERS\n",
    "###################\n",
    "# SET index_keep TO THE CLASS WANT TO SET AS 1, WANT THE NN TO LEARN - SET THE OTHER INDEX REMOVE \n",
    "### Learn when \"down\"\n",
    "# index_keep= 0\n",
    "# index_remove_a= 1\n",
    "# index_remove_b= 2\n",
    "\n",
    "### Learn when \"up\"\n",
    "index_keep= 2\n",
    "index_remove_a= 0\n",
    "index_remove_b= 1\n",
    "###################\n",
    "##### SET PARAMETERS\n",
    "###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9db662-5306-4859-856f-e962f1996b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print(\"Test data: 'nan' removed from begining and end\")\n",
    "lu.display_frequency_classes(targets, DOWN_PCTS_PREDICT, UP_PCTS_PREDICT)\n",
    "\n",
    "targets_frequency = Counter(targets)\n",
    "print(\"VALIDATE removing should be POSITIVE?\")\n",
    "count_remove_a= targets_frequency[index_remove_a] - targets_frequency[index_keep] + targets_frequency[index_keep] //2\n",
    "count_remove_b= targets_frequency[index_remove_b] - targets_frequency[index_keep] + targets_frequency[index_keep] //2\n",
    "print(f\"Removing {index_remove_a}: {count_remove_a}\")\n",
    "print(f\"Removing {index_remove_b}: {count_remove_b}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd505db-6ec0-43f7-a761-abcc231d549b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "##### SET PARAMETERS\n",
    "###################\n",
    "# DECISION SET: REMOVING?\n",
    "indexes_remove_a= []\n",
    "# 2024-03-01 Do not remove anything\n",
    "# if count_remove_a > 0:\n",
    "#     indexes_remove_a = get_indexes_value(targets, index_remove_a, count_remove_a)\n",
    "\n",
    "# DECISION SET: REMOVING?\n",
    "indexes_remove_b= []\n",
    "# 2024-03-01 Do not remove anything\n",
    "# if count_remove_b > 0:\n",
    "#     indexes_remove_b = get_indexes_value(targets, index_remove_b, count_remove_b)\n",
    "\n",
    "indexes_remove= indexes_remove_a + indexes_remove_b\n",
    "targets_clean= lu.remove_indexes(targets, indexes_remove)\n",
    "\n",
    "lu.display_frequency_classes(targets_clean, DOWN_PCTS_PREDICT, UP_PCTS_PREDICT)\n",
    "print(f\"Targets len: {len(targets)} Targets clean: {len(targets_clean)} Difference: {len(targets)-len(targets_clean)}\")\n",
    "\n",
    "inputs_clean = [lu.remove_indexes(input, indexes_remove) for input in inputs]    \n",
    "print(f\"targets_clean positions(Keep={index_keep})(First:{targets_clean.index(index_keep)},Last:-{targets_clean[::-1].index(index_keep)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39663265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets 'index_keep' as target = 1 and rest of indexes to target=0\n",
    "targets_binary= lu.convert_binary(targets_clean, index_keep)\n",
    "print(f\"targets_binary First {targets_binary.index(True)} and Last(counting from end) {targets_binary[::-1].index(True)} position with True\")\n",
    "print(f\"targets_binary len: {len(targets_binary)} Input clean[0]: {len(inputs_clean[0])} Input clean[-1]: {len(inputs_clean[-1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66b9612-5714-45ea-9237-af591665f186",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "inputs_tensor = torch.Tensor(inputs_clean)\n",
    "print(f\"inputs_tensor: {inputs_tensor.size()}\")\n",
    "inputs_tensor = inputs_tensor.T\n",
    "print(f\"inputs_tensor: {inputs_tensor.size()}\")\n",
    "targets_tensor = torch.Tensor(targets_binary)\n",
    "print(f\"inputs_clean len0 x len1: {len(inputs_clean)} x {len(inputs_clean[0])} -> inputs_tensor.shape: {inputs_tensor.shape}\")\n",
    "print(f\"targets_binary.shape: {len(targets_binary)} -> targets_tensor.shape: {targets_tensor.shape}\")\n",
    "print(f\"inputs_tensor: {inputs_tensor}\")\n",
    "print(f\"targets_tensor: {targets_tensor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509cc3f9-10c5-43d0-83a5-4224daee46f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inputs_tensor_test, targets_tensor_test= inputs_tensor, targets_tensor\n",
    "\n",
    "print(\"Test dataset frequencies:\")\n",
    "lu.display_frequency_values(targets_tensor_test.tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45cbcce-5556-49c1-8701-3b38771b10ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class StockDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        inputs,\n",
    "        targets):\n",
    "        \n",
    "        self.inputs= inputs\n",
    "        self.targets= targets\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, int]:\n",
    "        return self.inputs[index], self.targets[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3e3e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# TODO: When executing only using 33-38% GPU - Try different BATCH_SIZE see if parallelism increases? Learning decreases because less batches?\n",
    "BATCH_SIZE= 32\n",
    "\n",
    "test_dataset= StockDataset(\n",
    "  inputs_tensor_test,\n",
    "  targets_tensor_test\n",
    ")\n",
    "\n",
    "print(f\"First input vector:\\n{test_dataset[0]}\")\n",
    "\n",
    "test_dataloader= DataLoader(\n",
    "  dataset=test_dataset,\n",
    "  batch_size=BATCH_SIZE,\n",
    "  shuffle=False\n",
    ")\n",
    "\n",
    "test_input0, test_target0= next(iter(test_dataloader))\n",
    "print(f\"Dataloader batch={BATCH_SIZE}\\nInput:\\n{test_input0}\\nTargets:\\n{test_target0}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e14903",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "##### SET PARAMETERS\n",
    "###################\n",
    "HIDDEN_UNITS=12\n",
    "###################\n",
    "##### SET PARAMETERS\n",
    "###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd01423f-68e8-434f-8b20-bb0583bade18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXECUTE FROM THIS STEP To CREATE A NETWORK WITH RANDOM WEIGHTS\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class StockModelBinaryV0(nn.Module):\n",
    "  def __init__(self, input_features, hidden_units):\n",
    "    \"\"\"Initializes multi-class classification model\"\"\"\n",
    "    super().__init__()\n",
    "    self.linear_layer_stack = nn.Sequential(\n",
    "      nn.Linear(in_features=input_features, out_features=hidden_units*16),\n",
    "      nn.LeakyReLU(negative_slope=0.1),\n",
    "      nn.Linear(in_features=hidden_units*16, out_features=hidden_units*8),\n",
    "      nn.LeakyReLU(negative_slope=0.1),\n",
    "      nn.Linear(in_features=hidden_units*8, out_features=hidden_units*4),\n",
    "      nn.LeakyReLU(negative_slope=0.1),\n",
    "      nn.Linear(in_features=hidden_units*4, out_features=hidden_units),\n",
    "      nn.LeakyReLU(negative_slope=0.1),\n",
    "      nn.Linear(in_features=hidden_units, out_features=1)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    # print(\"forward x: \",\", \".join([str(num) for num in x.tolist()]))\n",
    "    # Layers are defined inside the Sequencial NN and will be applied here.\n",
    "    return self.linear_layer_stack(x)\n",
    "\n",
    "# Create an instance of the model\n",
    "model_0 = StockModelBinaryV0(\n",
    "  input_features=len(signal_avg),\n",
    "  hidden_units=HIDDEN_UNITS).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fd0792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads model from file\n",
    "from pathlib import Path\n",
    "\n",
    "# Create directory, if it doesn't exist, to store models\n",
    "MODEL_PATH= Path(\"models\")\n",
    "MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_NAME= f\"2024-05-15-1432-TSLA-predictUP-dates20170101-20240101-days5-down2-up5-in16-hid12-pos_weight7496pct-prec9956pct-fp9tp2017-model-best.pth\"\n",
    "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
    "\n",
    "model_0.load_state_dict(torch.load(f=MODEL_SAVE_PATH))\n",
    "model_0.to(device)\n",
    "\n",
    "print(f\"Test model loaded: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dab9b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_THRESHOLD = 0.9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fc3e0a-5ab5-4a72-a744-0d465dcbec94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import ConfusionMatrix, Accuracy, Precision\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "\n",
    "model_0.eval()\n",
    "test_precision= 0\n",
    "with torch.inference_mode():\n",
    "    X= inputs_tensor_test.to(device)\n",
    "    y= targets_tensor_test.to(device)\n",
    "\n",
    "    # Predict for test data\n",
    "    test_logits= model_0(X).view(-1)\n",
    "    sigmoid_output = torch.sigmoid(test_logits)\n",
    "    test_pred = (sigmoid_output > TEST_THRESHOLD).float()    \n",
    "\n",
    "confmat= ConfusionMatrix(task='binary')\n",
    "\n",
    "# test_data.targets are the values we want to predict in the test dataloader\n",
    "confmat_tensor= confmat(\n",
    "  preds= test_pred.cpu(),\n",
    "  target= targets_tensor_test.cpu())\n",
    "\n",
    "# Plot confusion matrix\n",
    "fig, ax= plot_confusion_matrix(\n",
    "  conf_mat= confmat_tensor.numpy(),\n",
    "  figsize= (10, 7)\n",
    ")\n",
    "\n",
    "accuracy_fn= Accuracy(task='binary').to(device)\n",
    "test_accuracy = accuracy_fn(test_pred, y)\n",
    "print(f\"Test threshold: {TEST_THRESHOLD}\")\n",
    "print(f\"Test confusion matrix:\\n{confmat_tensor}\")\n",
    "\n",
    "precision_fn= Precision(task='binary').to(device)\n",
    "test_precision = precision_fn(test_pred, y)\n",
    "print(f\"Test Accuracy: {test_accuracy*100:.2f}%\")\n",
    "print(f\"Test Precision: {test_precision*100:.2f}%\")\n",
    "false_positives = confmat_tensor[0, 1].item()\n",
    "true_positives = confmat_tensor[1, 1].item()\n",
    "print(f\"Test false_positives: {false_positives} true_positives: {true_positives}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a13af4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
