{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d615f90b",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "Trains a model on past data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a94b2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d154e01b-69f9-4b2e-adf2-34a7b8833a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e88ece4",
   "metadata": {},
   "source": [
    "### SET PARAMETERS:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff649bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "TICKER=\"TDOC\"\n",
    "DATA_INTERVAL_MINUTES = 15   # (Set to 5 or 15)\n",
    "DATA_AFTER_HOURS = False\n",
    "\n",
    "DAYS_PREDICT = 3\n",
    "# ISSUE: If stock goes down slowly, less than 'DOWN_PCTS_PREDICT' then won't sell but after few periods will be very down \n",
    "#  example: DOWN_PCTS_PREDICT=5% then down 4% and down 3% and down 1%... and never sell\n",
    "DOWN_PCTS_PREDICT= [3.0]\n",
    "UP_PCTS_PREDICT= [3.0]\n",
    "\n",
    "signal_avg= [\n",
    "    2, \n",
    "    3, \n",
    "    5, \n",
    "    8, \n",
    "    13, \n",
    "    21, \n",
    "    34, \n",
    "    55, \n",
    "    89, \n",
    "    144, \n",
    "    233, \n",
    "    377, \n",
    "    610, \n",
    "    987, \n",
    "    1597, \n",
    "    2584]\n",
    "\n",
    "PREDICT_UP = False\n",
    "if PREDICT_UP:\n",
    "    INDEX_KEEP= 2\n",
    "    INDEX_REMOVE_A= 0\n",
    "    INDEX_REMOVE_B= 1\n",
    "else:\n",
    "    INDEX_KEEP= 0\n",
    "    INDEX_REMOVE_A= 1\n",
    "    INDEX_REMOVE_B= 2\n",
    "\n",
    "TRAIN_SPLIT = 0.9\n",
    "\n",
    "# TODO: When executing only using 33-38% GPU - Try different BATCH_SIZE see if parallelism increases? Learning decreases because less batches?\n",
    "BATCH_SIZE= 32\n",
    "\n",
    "HIDDEN_UNITS=12\n",
    "\n",
    "TRAINING_THRESHOLD = 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7064ba",
   "metadata": {},
   "source": [
    "#### DOWNLOAD DATA (DON'T EXECUTE IF ALREADY LOADED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62afeeba-6a9c-41cf-bbf7-da672fc6ff5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..\\\\..')\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import settings\n",
    "import apis.tiingo_api as tiingo\n",
    "\n",
    "secret_key= settings.get_secret(\"tiingo-key\")\n",
    "\n",
    "### TRAINING DATA\n",
    "csv_data2017= tiingo.download_ticker(secret_key, TICKER, datetime(2017, 1, 1), datetime(2018,1,1), DATA_INTERVAL_MINUTES, DATA_AFTER_HOURS)\n",
    "csv_data2018= tiingo.download_ticker(secret_key, TICKER, datetime(2018, 1, 1), datetime(2019,1,1), DATA_INTERVAL_MINUTES, DATA_AFTER_HOURS)\n",
    "csv_data2019= tiingo.download_ticker(secret_key, TICKER, datetime(2019, 1, 1), datetime(2020,1,1), DATA_INTERVAL_MINUTES, DATA_AFTER_HOURS)\n",
    "csv_data2020= tiingo.download_ticker(secret_key, TICKER, datetime(2020, 1, 1), datetime(2021,1,1), DATA_INTERVAL_MINUTES, DATA_AFTER_HOURS)\n",
    "csv_data2021= tiingo.download_ticker(secret_key, TICKER, datetime(2021, 1, 1), datetime(2022,1,1), DATA_INTERVAL_MINUTES, DATA_AFTER_HOURS)\n",
    "csv_data2022= tiingo.download_ticker(secret_key, TICKER, datetime(2022, 1, 1), datetime(2023,1,1), DATA_INTERVAL_MINUTES, DATA_AFTER_HOURS)\n",
    "csv_data2023= tiingo.download_ticker(secret_key, TICKER, datetime(2023, 1, 1), datetime(2024,1,1), DATA_INTERVAL_MINUTES, DATA_AFTER_HOURS)\n",
    "# csv_data2024= tiingo.download_ticker(secret_key, TICKER, datetime(2024, 1, 1), datetime(2024,2,1), DATA_INTERVAL_MINUTES, DATA_AFTER_HOURS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a77e70-8b1d-4be0-9330-cfe1fe98ddaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import pandas as pd\n",
    "\n",
    "# ### TRAINING DATA\n",
    "df2017 = pd.read_csv(io.StringIO(csv_data2017))\n",
    "df2018 = pd.read_csv(io.StringIO(csv_data2018))\n",
    "df2019 = pd.read_csv(io.StringIO(csv_data2019))\n",
    "df2020 = pd.read_csv(io.StringIO(csv_data2020))\n",
    "df2021 = pd.read_csv(io.StringIO(csv_data2021))\n",
    "df2022 = pd.read_csv(io.StringIO(csv_data2022))\n",
    "df2023 = pd.read_csv(io.StringIO(csv_data2023))\n",
    "# df2024 = pd.read_csv(io.StringIO(csv_data2024))\n",
    "\n",
    "# if not df2017.empty:\n",
    "#     print(\"Concatenating from 2017\")\n",
    "#     df = pd.concat([df2017, df2018, df2019, df2020, df2021, df2022, df2023], axis=0, ignore_index=True)\n",
    "# elif not df2018.empty:\n",
    "#     print(\"Concatenating from 2018\")\n",
    "#     df = pd.concat([df2018, df2019, df2020, df2021, df2022, df2023], axis=0, ignore_index=True)\n",
    "# el\n",
    "if not df2019.empty:\n",
    "    print(\"Concatenating from 2019\")\n",
    "    df = pd.concat([df2019, df2020, df2021, df2022, df2023], axis=0, ignore_index=True)\n",
    "elif not df2020.empty:\n",
    "    print(\"Concatenating from 2020\")\n",
    "    df = pd.concat([df2020, df2021, df2022, df2023], axis=0, ignore_index=True)\n",
    "else:\n",
    "    print(\"Concatenating from 2021\")\n",
    "    df = pd.concat([df2021, df2022, df2023], axis=0, ignore_index=True)\n",
    "\n",
    "# if not df2024.empty:\n",
    "#     print(\"Concatenating from 2024\")\n",
    "#     df = pd.concat([df, df2024], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66b7cc7-98a6-43a0-82b1-0b2abdc83d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validates that data has been concatenated correctly = ordered ascending\n",
    "if df[\"date\"].is_monotonic_increasing and df[\"date\"].is_unique:\n",
    "    print(\"Correct: DataFrame is in ascending order.\")\n",
    "else:\n",
    "    print(\"Error: DataFrame is not in ascending order.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de91e28-bd1d-41bf-899b-4e183ae5be45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displays first and last element in the data\n",
    "print(f\"Data first:\\n{df[['date', 'close']][:5]}\")\n",
    "print(f\"Data last:\\n{df[['date', 'close']][-5:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e990fa6-00fc-496d-be86-955866c3ffb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If quotes every 15min there 26 per day if quotes every 5min there are 78 per day\n",
    "TICKS_IN_DAY = 26 if DATA_INTERVAL_MINUTES == 15 else 78\n",
    "# How many data ticks are inspecting to determine the if up or down by percentage \n",
    "TICKS_PREDICT= TICKS_IN_DAY * DAYS_PREDICT\n",
    "REACH_PCT= 0.95\n",
    "\n",
    "import classifiers.up_down_classifier as udc\n",
    "import classifiers.ewa_classifier as ec\n",
    "\n",
    "alpha= ec.calculate_ewa_alpha(TICKS_PREDICT, REACH_PCT)\n",
    "print(f\"alpha: {alpha:.4f} for window: {TICKS_PREDICT} and reach: {REACH_PCT}\")\n",
    "\n",
    "classes_calc = udc.UpsDownsClassifier(TICKS_PREDICT, DOWN_PCTS_PREDICT, UP_PCTS_PREDICT)\n",
    "\n",
    "close_prices = df['close'].astype(float).tolist()\n",
    "input_data= ec.calculate_ewas(close_prices, alpha)\n",
    "\n",
    "classes= classes_calc.classify(input_data)\n",
    "print(f\"Check correct 'nan' point (window={TICKS_PREDICT}): {classes[-TICKS_PREDICT-1:-TICKS_PREDICT+1]}\")\n",
    "print(f\"prices vs input_data: {[(p, c) for p, c in zip(close_prices[2650:3000], input_data[2650:3000])]}\")\n",
    "print(f\"input_data vs classes: {[(p, c) for p, c in zip(input_data[2650:3000], classes[2650:3000])]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a25bdb-a1ac-49b9-b508-55aa08e54c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a histogram\n",
    "hist_values, bin_edges, _ = plt.hist(classes, bins=3, edgecolor='black')\n",
    "\n",
    "# Add labels and a title\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Data')\n",
    "\n",
    "# Display frequency on top of each bar\n",
    "for value, edge in zip(hist_values, bin_edges[:-1]):\n",
    "    plt.text(float(edge), float(value), str(int(value)), color='black')\n",
    "    \n",
    "# Show the histogram\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d689836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show percentages of each class value\n",
    "import utils.list_utils as lu\n",
    "\n",
    "lu.display_frequency_classes(classes, DOWN_PCTS_PREDICT, UP_PCTS_PREDICT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8ce196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display classes value changes over time (last 500 ticks)\n",
    "graph_ticks = 500\n",
    "x = range(len(classes[-graph_ticks:]))\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(x, classes[-graph_ticks:], linestyle='-')\n",
    "\n",
    "\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Class')\n",
    "plt.title('Plot of Classes')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1347910b-5c73-4c91-847e-906b66ad4e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate the signals as input for the neural network as proportions\n",
    "import preprocessing.proportions_calc as proportions\n",
    "\n",
    "signals_calculator = proportions.ProportionsCalc(signal_avg)\n",
    "\n",
    "proportions_avg = signals_calculator.calculate(close_prices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ac18bb-6cb7-4307-ab28-e8b5d5f2a74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Prices length: {len(close_prices)}\")\n",
    "print(f\"Proportions length: {len(proportions_avg[-1])}\")\n",
    "\n",
    "print(f\"Last 10 close: {close_prices[-10:]}\")\n",
    "print(f\"Last 10 proportions(avg={signal_avg[0]}): {proportions_avg[0][-10:]}\")\n",
    "\n",
    "print(f\"Proportions avgs: Count: {len(signal_avg)} Max: {signal_avg[-1]}\")\n",
    "# At the end of the data, when less ticks than necessary no possible to predict so \"nan\" \n",
    "print(f\"Classes last non-nan: {classes[-TICKS_PREDICT-1:-TICKS_PREDICT+1]} len: {len(classes)}\")\n",
    "print(f\"Proportions first non-nan(avg={signal_avg[-1]}): {proportions_avg[-1][signal_avg[-1]-2:signal_avg[-1]]} len: {len(proportions_avg[-1])}\")\n",
    "print(f\"Proportions (avg={signal_avg[0]}) Min: {min(proportions_avg[0][signal_avg[0]-1:-TICKS_PREDICT-1])} Max: {max(proportions_avg[0][signal_avg[0]-1:-TICKS_PREDICT])}\")\n",
    "print(f\"Proportions (avg={signal_avg[-1]}) Min: {min(proportions_avg[-1][signal_avg[-1]-1:-TICKS_PREDICT-1])} Max: {max(proportions_avg[-1][signal_avg[-1]-1:-TICKS_PREDICT])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bb2e59-3b6b-4691-8d9e-a5b0754c6f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the \"nan\" from the proportions\n",
    "#   At the beging first signal_avg[-1] are \"nan\" (need previous values for first avg.)\n",
    "#   At the end decided not predict if period to predict is shorter\n",
    "targets = classes[signal_avg[-1]-1:-TICKS_PREDICT]\n",
    "inputs = []\n",
    "for proportion in proportions_avg:\n",
    "    proportion_cut= proportion[signal_avg[-1]-1:-TICKS_PREDICT]\n",
    "    print(proportion_cut[:2])\n",
    "    inputs.append(proportion_cut)\n",
    "\n",
    "print(f\"First target: {targets[0]} and last target: {targets[-1]}\")\n",
    "print(f\"Classes: {len(classes)} after cut to targets: {len(targets)}\")\n",
    "print(f\"Inputs len: {len(inputs[len(signal_avg)-1])}\")\n",
    "print(f\"Distinct targets: {list(set(targets))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9db662-5306-4859-856f-e962f1996b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print(\"Train data: 'nan' removed from begining and end\")\n",
    "lu.display_frequency_classes(targets, DOWN_PCTS_PREDICT, UP_PCTS_PREDICT)\n",
    "\n",
    "targets_frequency = Counter(targets)\n",
    "print(\"VALIDATE removing should be POSITIVE?\")\n",
    "count_remove_a= targets_frequency[INDEX_REMOVE_A] - targets_frequency[INDEX_KEEP] + targets_frequency[INDEX_KEEP] //2\n",
    "count_remove_b= targets_frequency[INDEX_REMOVE_B] - targets_frequency[INDEX_KEEP] + targets_frequency[INDEX_KEEP] //2\n",
    "print(f\"Removing {INDEX_REMOVE_A}: {count_remove_a}\")\n",
    "print(f\"Removing {INDEX_REMOVE_B}: {count_remove_b}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd505db-6ec0-43f7-a761-abcc231d549b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "##### SET PARAMETERS\n",
    "###################\n",
    "# DECISION SET: REMOVING?\n",
    "indexes_remove_a= []\n",
    "# 2024-03-01 Do not remove anything\n",
    "# if count_remove_a > 0:\n",
    "#     indexes_remove_a = get_indexes_value(targets, index_remove_a, count_remove_a)\n",
    "\n",
    "# DECISION SET: REMOVING?\n",
    "indexes_remove_b= []\n",
    "# 2024-03-01 Do not remove anything\n",
    "# if count_remove_b > 0:\n",
    "#     indexes_remove_b = get_indexes_value(targets, index_remove_b, count_remove_b)\n",
    "\n",
    "indexes_remove= indexes_remove_a + indexes_remove_b\n",
    "targets_clean= lu.remove_indexes(targets, indexes_remove)\n",
    "\n",
    "lu.display_frequency_classes(targets_clean, DOWN_PCTS_PREDICT, UP_PCTS_PREDICT)\n",
    "print(f\"Targets len: {len(targets)} Targets clean: {len(targets_clean)} Difference: {len(targets)-len(targets_clean)}\")\n",
    "\n",
    "inputs_clean = [lu.remove_indexes(input, indexes_remove) for input in inputs]    \n",
    "print(f\"targets_clean positions(Keep={INDEX_KEEP})(First:{targets_clean.index(INDEX_KEEP)},Last:-{targets_clean[::-1].index(INDEX_KEEP)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39663265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets 'index_keep' as target = 1 and rest of indexes to target=0\n",
    "targets_binary= lu.convert_binary(targets_clean, INDEX_KEEP)\n",
    "print(f\"targets_binary First {targets_binary.index(True)} and Last(counting from end) {targets_binary[::-1].index(True)} position with True\")\n",
    "print(f\"targets_binary len: {len(targets_binary)} Input clean[0]: {len(inputs_clean[0])} Input clean[-1]: {len(inputs_clean[-1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66b9612-5714-45ea-9237-af591665f186",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "inputs_tensor = torch.Tensor(inputs_clean)\n",
    "print(f\"inputs_tensor: {inputs_tensor.size()}\")\n",
    "inputs_tensor = inputs_tensor.T\n",
    "print(f\"inputs_tensor: {inputs_tensor.size()}\")\n",
    "targets_tensor = torch.Tensor(targets_binary)\n",
    "print(f\"inputs_clean len0 x len1: {len(inputs_clean)} x {len(inputs_clean[0])} -> inputs_tensor.shape: {inputs_tensor.shape}\")\n",
    "print(f\"targets_binary.shape: {len(targets_binary)} -> targets_tensor.shape: {targets_tensor.shape}\")\n",
    "print(f\"inputs_tensor: {inputs_tensor}\")\n",
    "print(f\"targets_tensor: {targets_tensor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a759cdf-b8e5-4f12-8b93-49d296c9b403",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffle tensors\n",
    "torch.manual_seed(42) \n",
    "permutation = torch.randperm(inputs_tensor.size(0))\n",
    "inputs_tensor_shuffle = inputs_tensor[permutation]\n",
    "\n",
    "targets_tensor_shuffle = targets_tensor[permutation]\n",
    "\n",
    "print(f\"inputs_tensor.size(0): {inputs_tensor.size(0)}\")\n",
    "print(f\"inputs_tensor.shape: {inputs_tensor.shape} -> inputs_tensor_shuffle.shape: {inputs_tensor_shuffle.shape}\")\n",
    "print(f\"targets_tensor.shape: {targets_tensor.shape} -> targets_tensor_shuffle.shape: {targets_tensor_shuffle.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509cc3f9-10c5-43d0-83a5-4224daee46f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_test_split= int(TRAIN_SPLIT * len(targets_tensor))\n",
    "inputs_tensor_train, targets_tensor_train= inputs_tensor_shuffle[:train_test_split], targets_tensor_shuffle[:train_test_split]\n",
    "inputs_tensor_test, targets_tensor_test= inputs_tensor_shuffle[train_test_split:], targets_tensor_shuffle[train_test_split:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89daca86-671c-4257-ba5b-93807f06edbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training dataset frequencies:\")\n",
    "lu.display_frequency_values(targets_tensor_train.tolist())\n",
    "print(\"Validation dataset frequencies:\")\n",
    "lu.display_frequency_values(targets_tensor_test.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45cbcce-5556-49c1-8701-3b38771b10ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class StockDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        inputs,\n",
    "        targets):\n",
    "        \n",
    "        self.inputs= inputs\n",
    "        self.targets= targets\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, int]:\n",
    "        return self.inputs[index], self.targets[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1e55d5-e9e3-4a93-af46-f8ec7eb92f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "train_dataset= StockDataset(\n",
    "  inputs_tensor_train,\n",
    "  targets_tensor_train\n",
    ")\n",
    "\n",
    "print(f\"train_dataset: {train_dataset[0]}\")\n",
    "\n",
    "train_dataloader= DataLoader(\n",
    "  dataset=train_dataset,\n",
    "  batch_size=BATCH_SIZE,\n",
    "  shuffle=False\n",
    ")\n",
    "\n",
    "train_input0, train_target0= next(iter(train_dataloader))\n",
    "print(f\"train_input0: {train_input0} train_target0: {train_target0}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3e3e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset= StockDataset(\n",
    "  inputs_tensor_test,\n",
    "  targets_tensor_test\n",
    ")\n",
    "\n",
    "print(f\"train_dataset: {train_dataset[0]}\")\n",
    "\n",
    "val_dataloader= DataLoader(\n",
    "  dataset=test_dataset,\n",
    "  batch_size=BATCH_SIZE,\n",
    "  shuffle=False\n",
    ")\n",
    "\n",
    "test_input0, test_target0= next(iter(val_dataloader))\n",
    "print(f\"test_input0: {train_input0} test_target0: {test_target0}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd01423f-68e8-434f-8b20-bb0583bade18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXECUTE FROM THIS STEP To CREATE A NETWORK WITH RANDOM WEIGHTS\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class StockModelBinaryV0(nn.Module):\n",
    "  def __init__(self, input_features, hidden_units):\n",
    "    \"\"\"Initializes multi-class classification model\"\"\"\n",
    "    super().__init__()\n",
    "    self.linear_layer_stack = nn.Sequential(\n",
    "      nn.Linear(in_features=input_features, out_features=hidden_units*16),\n",
    "      nn.LeakyReLU(negative_slope=0.1),\n",
    "      nn.Linear(in_features=hidden_units*16, out_features=hidden_units*8),\n",
    "      nn.LeakyReLU(negative_slope=0.1),\n",
    "      nn.Linear(in_features=hidden_units*8, out_features=hidden_units*4),\n",
    "      nn.LeakyReLU(negative_slope=0.1),\n",
    "      nn.Linear(in_features=hidden_units*4, out_features=hidden_units),\n",
    "      nn.LeakyReLU(negative_slope=0.1),\n",
    "      nn.Linear(in_features=hidden_units, out_features=1)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    # print(\"forward x: \",\", \".join([str(num) for num in x.tolist()]))\n",
    "    # Layers are defined inside the Sequencial NN and will be applied here.\n",
    "    return self.linear_layer_stack(x)\n",
    "\n",
    "# Create an instance of the model\n",
    "model_0 = StockModelBinaryV0(\n",
    "  input_features=len(signal_avg),\n",
    "  hidden_units=HIDDEN_UNITS).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293a691f-abc2-40a4-99d9-d6b705e9257d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "\n",
    "# loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Loss function for an imbalanced dataset (there many more 0's than 1's). Apply more weight to the less frequent class\n",
    "num_ones = torch.count_nonzero(targets_tensor_train)\n",
    "num_zeros = len(targets_tensor_train)-num_ones\n",
    "pos_weight = num_zeros / num_ones\n",
    "print(pos_weight)\n",
    "# pos_weight_tensor = torch.tensor([pos_weight]).to(device)\n",
    "# pos_weight_tensor = torch.tensor([1.5]).to(device)\n",
    "print(f\"Train negative: {num_zeros} positive: {num_ones} pos_weight: {pos_weight}\")\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e5eb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "##### SET PARAMETERS\n",
    "###################\n",
    "# PERFORMANCE_MEASURE=\"accu\"\n",
    "# performance_fn= torchmetrics.Accuracy(task='binary').to(device)\n",
    "PERFORMANCE_MEASURE=\"prec\"\n",
    "performance_fn= torchmetrics.Precision(task='binary').to(device)\n",
    "# PERFORMANCE_MEASURE=\"reca\"\n",
    "# performance_fn= torchmetrics.Recall(task='binary').to(device)\n",
    "###################\n",
    "##### SET PARAMETERS\n",
    "###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef281180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should you reset BEST PERFORMANCE\n",
    "best_val_performance = 0\n",
    "model_best = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adf8712",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "##### SET PARAMETERS\n",
    "###################\n",
    "# DO: AFTER THIS CELL RUNS EXECUTE CELLS UNTIL SAVE STEP TO KEEP BEST RESULT IN CASE IT GOES DOWN\n",
    "#    lr = 0.1 -> 0.03 -> 0.001\n",
    "#    epochs 200 + 200 (lr=0.1) -> 100 (lr=0.03) -> 100 (lr=0.001)\n",
    "# EXECUTE 0.1 x 200 for 2 TIMES (or 400 for 1 time)\n",
    "# optimizer = torch.optim.SGD(params=model_0.parameters(), lr=0.1)\n",
    "# epochs=400\n",
    "# optimizer = torch.optim.SGD(params=model_0.parameters(), lr=0.03)\n",
    "# epochs=100\n",
    "# optimizer = torch.optim.SGD(params=model_0.parameters(), lr=0.005)\n",
    "# epochs=100\n",
    "# optimizer = torch.optim.SGD(params=model_0.parameters(), lr=0.0007)\n",
    "# epochs=100\n",
    "# optimizer = torch.optim.SGD(params=model_0.parameters(), lr=0.0001)\n",
    "# epochs=100\n",
    "###Using Adam optimizer\n",
    "# learning_rate = 0.005\n",
    "# beta_1 = 0.9\n",
    "# beta_2 = 0.999\n",
    "# decay = 0.01\n",
    "# optimizer = optim.Adam(params=model_0.parameters(), lr=learning_rate, betas=(beta_1, beta_2), eps=1e-8, weight_decay=decay)\n",
    "# epochs=2000\n",
    "\n",
    "EPOCHS=1000\n",
    "LEARNING_RATE= 0.1\n",
    "optimizer = torch.optim.SGD(params=model_0.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "##### Using a StepLR Scheduler\n",
    "# from torch.optim.lr_scheduler import StepLR\n",
    "# gamma = 0.95\n",
    "# scheduler = StepLR(optimizer, step_size=20, gamma=gamma)\n",
    "## If continue optimization\n",
    "# learning_rate_last= 0.0080995 * 0.95\n",
    "# gamma = 0.95\n",
    "# optimizer = torch.optim.SGD(params=model_0.parameters(), lr=learning_rate_last*gamma)\n",
    "# scheduler = StepLR(optimizer, step_size=20, gamma=gamma)\n",
    "# epochs= 200\n",
    "\n",
    "##### Using a ReduceLROnPlateau\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "###################\n",
    "##### SET PARAMETERS\n",
    "###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8757655-3fc1-4b53-93da-5bb94f4ed651",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "EARLY_STOPPING_PATIENCE = 60\n",
    "\n",
    "best_model_keep = \"high_prec\"\n",
    "best_val_loss = float('inf')\n",
    "early_stopping_counter = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_performance= 0, 0\n",
    "    train_samples = 0\n",
    "\n",
    "    # Training\n",
    "    model_0.train()\n",
    "\n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "        X= X.to(device)\n",
    "        y= y.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        y_logits= model_0(X).view(-1)\n",
    "        \n",
    "        # turn logits -> prediction probabilities -> prediction labels\n",
    "        y_sigmoid_output = torch.sigmoid(y_logits)\n",
    "        y_pred = (y_sigmoid_output > TRAINING_THRESHOLD).float()\n",
    "        \n",
    "        # Calculate loss and accuracy\n",
    "        loss= loss_fn(y_logits, y)\n",
    "        train_loss+= loss * X.size(0)\n",
    "        train_performance+= performance_fn(y_pred, y) * X.size(0)\n",
    "        train_samples += X.size(0)\n",
    "        \n",
    "        # Zero the gradients to avoid accomulating gradients from previous iteration\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # Updates the model usign the gradients\n",
    "        optimizer.step()\n",
    "    \n",
    "    train_loss /= train_samples\n",
    "    train_performance /= train_samples\n",
    "      \n",
    "    model_0.eval()\n",
    "    val_loss, val_performance= 0, 0\n",
    "    val_samples = 0\n",
    "    with torch.inference_mode():\n",
    "        for X, y in val_dataloader:\n",
    "            X= X.to(device)\n",
    "            y= y.to(device)\n",
    "        \n",
    "            # Predict for test data\n",
    "            val_logits= model_0(X).view(-1)\n",
    "            sigmoid_output = torch.sigmoid(val_logits)\n",
    "            val_pred = (sigmoid_output > TRAINING_THRESHOLD).float()\n",
    "            \n",
    "            # Calculate test loss/accuracy\n",
    "            val_loss+= loss_fn(val_logits, y) * X.size(0)\n",
    "            val_performance+= performance_fn(val_pred, y) * X.size(0)\n",
    "            val_samples += X.size(0)\n",
    "\n",
    "        val_loss /= val_samples\n",
    "        val_performance /= val_samples\n",
    "    \n",
    "    # Update the learning rate?\n",
    "    if scheduler.__class__.__name__ == \"ReduceLROnPlateau\":\n",
    "        scheduler.step(val_loss)\n",
    "    else:\n",
    "        scheduler.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "       print(f\"Epoch: {epoch+1} lr: {optimizer.param_groups[0]['lr']:.7f} | Loss: {train_loss:.5f} {PERFORMANCE_MEASURE}: {train_performance*100:.4f}% | Val loss: {val_loss:.5f} Val {PERFORMANCE_MEASURE}: {val_performance*100:.4f}%\")\n",
    "\n",
    "    ## Store model with higher precision\n",
    "    if best_model_keep == \"high_prec\" and val_performance > best_val_performance:\n",
    "        best_val_performance = val_performance\n",
    "        model_best = deepcopy(model_0)\n",
    "            \n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        early_stopping_counter = 0\n",
    "        #Store model with lower validation loss?\n",
    "        if best_model_keep == \"lower_loss\":\n",
    "            model_best = deepcopy(model_0)\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "        if early_stopping_counter >= EARLY_STOPPING_PATIENCE:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break    \n",
    "\n",
    "print(f\"Finished: Epoch: {epoch+1} lr: {optimizer.param_groups[0]['lr']:.7f} | Loss: {train_loss:.5f} {PERFORMANCE_MEASURE}: {train_performance*100:.4f}% | Val loss: {val_loss:.5f} Val {PERFORMANCE_MEASURE}: {val_performance*100:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fc3e0a-5ab5-4a72-a744-0d465dcbec94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import ConfusionMatrix, Accuracy, Precision\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "\n",
    "model_best.eval()\n",
    "val_loss, val_performance= 0, 0\n",
    "with torch.inference_mode():\n",
    "    X= inputs_tensor_test.to(device)\n",
    "    y= targets_tensor_test.to(device)\n",
    "\n",
    "    # Predict for validation data\n",
    "    val_logits= model_best(X).view(-1)\n",
    "    sigmoid_output = torch.sigmoid(val_logits)\n",
    "    val_pred = (sigmoid_output > TRAINING_THRESHOLD).float()\n",
    "    \n",
    "    # Calculate loss/performance(accuracy|precision)\n",
    "    val_loss+= loss_fn(val_logits, y)\n",
    "    val_performance+= performance_fn(val_pred, y)\n",
    "\n",
    "print(f\"Validation loss: {val_loss:.5f} Performance {PERFORMANCE_MEASURE}: {val_performance*100:.7f}%\")\n",
    "\n",
    "confmat= ConfusionMatrix(task='binary')\n",
    "\n",
    "# test_data.targets are the values we want to predict in the test dataloader\n",
    "confmat_tensor= confmat(\n",
    "  preds= val_pred.cpu(),\n",
    "  target= targets_tensor_test.cpu())\n",
    "\n",
    "# Plot confusion matrix\n",
    "fig, ax= plot_confusion_matrix(\n",
    "  conf_mat= confmat_tensor.numpy(),\n",
    "  figsize= (10, 7)\n",
    ")\n",
    "\n",
    "accuracy_fn= Accuracy(task='binary').to(device)\n",
    "val_accuracy = accuracy_fn(val_pred, y)\n",
    "print(f\"Train validation confusion matrix:\\n{confmat_tensor}\")\n",
    "\n",
    "precision_fn= Precision(task='binary').to(device)\n",
    "val_precision = precision_fn(val_pred, y)\n",
    "print(f\"Train validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Train validation precision: {val_precision*100:.2f}%\")\n",
    "false_positives = confmat_tensor[0, 1].item()\n",
    "true_positives = confmat_tensor[1, 1].item()\n",
    "print(f\"Train validation false_positives: {false_positives} true_positives: {true_positives}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b121c6be",
   "metadata": {},
   "source": [
    "#### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e8cb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.text_utils as tu\n",
    "\n",
    "date_start= df['date'][0]\n",
    "date_end= df['date'].iloc[-1]\n",
    "\n",
    "target= \"UP\" if PREDICT_UP else \"DOWN\"\n",
    "MODEL_NAME= f\"{datetime.now().strftime('%Y-%m-%d-%H%M')}-{TICKER}-predict{target}-dates{tu.shorten_date(date_start)}-{tu.shorten_date(date_end)}-days{DAYS_PREDICT}-down{int(DOWN_PCTS_PREDICT[0]*100)}-up{int(UP_PCTS_PREDICT[0]*100)}-in{len(signal_avg)}-hid{HIDDEN_UNITS}-pos_weight{pos_weight*10000:.0f}-{PERFORMANCE_MEASURE}{val_performance*10000:.0f}pct-fp{false_positives}tp{true_positives}-{best_model_keep}.pth\"\n",
    "\n",
    "print(\"======TRAINING:\")\n",
    "print(f\"Ticker: {TICKER}\")\n",
    "\n",
    "print(\"--Data\")\n",
    "print(f\"Start: {date_start} End: {date_end}\")\n",
    "print(f\"Interval: {DATA_INTERVAL_MINUTES} - After Hours: {DATA_AFTER_HOURS}\")\n",
    "print(\"Targets Frequencies:\")\n",
    "lu.display_frequency_classes(targets_clean, DOWN_PCTS_PREDICT, UP_PCTS_PREDICT)\n",
    "\n",
    "print(f\"Signal Averages: {signal_avg}\")\n",
    "print(f\"Train predict {'UP' if PREDICT_UP else 'DOWN'} - Days: {DAYS_PREDICT} Down pcts: {DOWN_PCTS_PREDICT} Up pcts: {UP_PCTS_PREDICT}\")\n",
    "\n",
    "print(\"Training dataset frequencies:\")\n",
    "lu.display_frequency_values(targets_tensor_train.tolist())\n",
    "print(\"Validation dataset frequencies:\")\n",
    "lu.display_frequency_values(targets_tensor_test.tolist())\n",
    "\n",
    "print(\"--Training\")\n",
    "print(f\"Network hidden units: {HIDDEN_UNITS}\")\n",
    "print(f\"Loss func: {loss_fn.__class__.__name__}\")\n",
    "print(f\"Train negative: {num_zeros} positive: {num_ones} pos_weight: {pos_weight}\")\n",
    "print(f\"Optimizer: {optimizer.__class__.__name__} Scheduler: {scheduler.__class__.__name__}\")\n",
    "print(f\"Train/Val split: {TRAIN_SPLIT}\")\n",
    "print(f\"Train threshold: {TRAINING_THRESHOLD}\")\n",
    "print(f\"Epochs: {EPOCHS} learning_rate: {LEARNING_RATE}\")\n",
    "\n",
    "print(\"--Validation Results\")\n",
    "print(f\"Trained model: {MODEL_NAME}\")\n",
    "print(f\"Validation loss: {val_loss:.5f} Performance {PERFORMANCE_MEASURE}: {val_performance*100:.2f}%\")\n",
    "print(f\"Train validation confusion matrix:\\n{confmat_tensor}\")\n",
    "print(f\"Train validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Train validation precision: {val_precision*100:.2f}%\")\n",
    "print(f\"Train validation false_positives: {false_positives} true_positives: {true_positives}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501c5bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves model.state_dic() with best performance to a file\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Create directory, if it doesn't exist, to store models\n",
    "MODEL_PATH= Path(\"models\")\n",
    "MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create path to the model\n",
    "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
    "\n",
    "# torch.save(\n",
    "#   obj=model_0.state_dict(),\n",
    "#   f=f\"{MODEL_SAVE_PATH}.pth\")\n",
    "\n",
    "torch.save(\n",
    "    obj=model_best.state_dict(), \n",
    "    f=f\"{MODEL_SAVE_PATH}\")\n",
    "\n",
    "\n",
    "print(f\"Trained model saved: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afa50f8-6e03-45fd-8f74-6f47da208d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  import torchinfo\n",
    "except:\n",
    "  !pip install torchinfo\n",
    "  import torchinfo\n",
    "\n",
    "from torchinfo import summary  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5025345-72b4-4170-90c8-307279d139ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model_0, input_size=[len(signal_avg)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
